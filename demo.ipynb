{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef47729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from vgg_cifar import vgg\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "#import wandb\n",
    "#################### Random Seed 고정 ####################\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "##########################################################\n",
    "\n",
    "model_names = sorted(name for name in vgg.__dict__\n",
    "    if name.islower() and not name.startswith(\"__\")\n",
    "                     and name.startswith(\"vgg\")\n",
    "                     and callable(vgg.__dict__[name]))\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch VGG Trainer')\n",
    "parser.add_argument('-a', '--arch', metavar='ARCH', default='vgg19_bn',\n",
    "                    choices=model_names,\n",
    "                    help='model architecture: ' + ' | '.join(model_names) +\n",
    "                    ' (default: vgg19)')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('--epochs', default=300, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('-b', '--batch-size', default=128, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 128)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.05, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, metavar='M',\n",
    "                    help='momentum')\n",
    "parser.add_argument('--weight-decay', '--wd', default=5e-4, type=float,\n",
    "                    metavar='W', help='weight decay (default: 5e-4)')\n",
    "parser.add_argument('--print-freq', '-p', default=100, type=int,\n",
    "                    metavar='N', help='print frequency (default: 20)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\n",
    "                    help='evaluate model on validation set')\n",
    "parser.add_argument('--pretrained', dest='pretrained', action='store_true',\n",
    "                    help='use pre-trained model')\n",
    "parser.add_argument('--half', dest='half', action='store_true',\n",
    "                    help='use half-precision(16-bit) ')\n",
    "parser.add_argument('--cpu', dest='cpu', action='store_true',\n",
    "                    help='use cpu')\n",
    "parser.add_argument('--save-dir', dest='save_dir',\n",
    "                    help='The directory used to save the trained models',\n",
    "                    default='save_temp', type=str)\n",
    "parser.add_argument('--dataset', help='choose one of dataset : cifar10 or cifar100', default='cifar10', type=str)\n",
    "parser.add_argument('--block', help='block_type', default='VGG19', type=str)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "if args.dataset == \"cifar10\" :\n",
    "    num_classes = 10\n",
    "elif args.dataset == \"cifar100\" :\n",
    "    num_classes = 100\n",
    "print(\"dataset : \", args.dataset)\n",
    "print(\"num classes : \", num_classes)\n",
    "\n",
    "# Check the save_dir exists or not\n",
    "#save_path = os.path.join(args.save_dir, args.dataset, args.block)\n",
    "#if not os.path.exists(save_path):\n",
    "#    os.makedirs(save_path)\n",
    "\n",
    "\n",
    "# cudnn.benchmark = False\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "if args.dataset == \"cifar10\":\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, 4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]), download=True),\n",
    "    batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])),\n",
    "    batch_size=args.batch_size, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True)\n",
    "elif args.dataset == \"cifar100\" :\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(root='./data', train=True, transform=transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(32, 4),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ]), download=True),\n",
    "    batch_size=args.batch_size, shuffle=True,\n",
    "    num_workers=args.workers, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR100(root='./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])),\n",
    "    batch_size=args.batch_size, shuffle=False,\n",
    "    num_workers=args.workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36675d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "        Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        if args.cpu == False:\n",
    "            input = input.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "        if args.half:\n",
    "            input = input.half()\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        \n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      data_time=data_time, loss=losses, top1=top1))\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        if args.cpu == False:\n",
    "            input = input.cuda(non_blocking=True)\n",
    "            target = target.cuda(non_blocking=True)\n",
    "\n",
    "        if args.half:\n",
    "            input = input.half()\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        \n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'\n",
    "          .format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"\n",
    "    Save the training model\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 2 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.5 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a55849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function (criterion) and pptimizer      \n",
    "def run_model(model):\n",
    "    if args.cpu:\n",
    "        model.cpu()\n",
    "    else:\n",
    "        model.cuda()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if args.cpu:\n",
    "        criterion = criterion.cpu()\n",
    "    else:\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    if args.half:\n",
    "        model.half()\n",
    "        criterion.half()\n",
    "\n",
    "    best_prec1 = 0\n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.weight_decay)\n",
    "    if args.evaluate:\n",
    "        validate(val_loader, model, criterion)\n",
    "\n",
    "    test_accuracy = []\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        #save_checkpoint({\n",
    "        #    'epoch': epoch + 1,\n",
    "        #    'state_dict': model.state_dict(),\n",
    "        #    'best_prec1': best_prec1,\n",
    "        #}, is_best, filename=os.path.join(save_path, 'checkpoint_{}.tar'.format(epoch)))\n",
    "        test_accuracy.append(prec1)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be4b47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.block = \"VGG19\"\n",
    "model = vgg.__dict__[args.arch](num_classes, args.block)\n",
    "model.features = torch.nn.DataParallel(model.features)\n",
    "\n",
    "vgg19_accuracy = run_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4d2eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.block = \"SE_SA_1\"\n",
    "model = vgg.__dict__[args.arch](num_classes, args.block)\n",
    "model.features = torch.nn.DataParallel(model.features)\n",
    "se_sa_accuracy = run_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11278f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.block = \"SEC_SA_1\"\n",
    "model = vgg.__dict__[args.arch](num_classes, args.block)\n",
    "model.features = torch.nn.DataParallel(model.features)\n",
    "sec_sa_accuracy = run_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f54647",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.block = \"CBAM_1\"\n",
    "model = vgg.__dict__[args.arch](num_classes, args.block)\n",
    "model.features = torch.nn.DataParallel(model.features)\n",
    "cbam_accuracy = run_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be0d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.block = \"NEW_1\"\n",
    "model = vgg.__dict__[args.arch](num_classes, args.block)\n",
    "model.features = torch.nn.DataParallel(model.features)\n",
    "ours_accuracy = run_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7726d369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(vgg19_accuracy, color = 'r')\n",
    "plt.plot(se_sa_accuracy, color = 'y')\n",
    "plt.plot(sec_sa_accuracy, color = 'b')\n",
    "plt.plot(cbam_accuracy, color = 'm')\n",
    "plt.plot(ours_accuracy, color = 'g')\n",
    "#plt.xlim([0, 5])      # X축의 범위: [xmin, xmax]\n",
    "#plt.ylim([0, 20])     # Y축의 범위: [ymin, ymax]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c908e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from resnet_cifar.models import *\n",
    "from resnet_cifar.resnet import *\n",
    "from resnet_cifar.utils import progress_bar\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#import wandb\n",
    "\n",
    "#################### Random Seed 고정 ####################\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "##########################################################\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "parser = argparse.ArgumentParser(description='PyTorch ResNet Training')\n",
    "parser.add_argument('--lr', default=0.1, type=float, help='learning rate')\n",
    "parser.add_argument('--resume', '-r', action='store_true',\n",
    "                        help='resume from checkpoint')\n",
    "parser.add_argument('--epochs', default=200, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "parser.add_argument('--dataset', help='dataset', default='cifar100', type=str)\n",
    "parser.add_argument('--block', help='block type', default='RESNET', type=str)\n",
    "parser.add_argument('--save_dir', default='save_temp', type=str)\n",
    "parser.add_argument('--print-freq', '-p', default=100, type=int,\n",
    "                    metavar='N', help='print frequency (default: 20)')\n",
    "\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3907584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "#            'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "if args.dataset == 'cifar10':\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "            trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "            root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "            testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "else:\n",
    "    trainset = torchvision.datasets.CIFAR100(\n",
    "            root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "            trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "    testset = torchvision.datasets.CIFAR100(\n",
    "            root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "            testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model\n",
    "#print('==> Building model..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456b6b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(epoch, trainloader, net, criterion, optimizer):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    last_idx = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        last_idx = batch_idx\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        #progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        #             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "        if batch_idx % args.print_freq == 0:\n",
    "            print(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                             % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "\n",
    "    #wandb.log({\n",
    "    #    'train_acc': 100.*correct/total,\n",
    "    #    'train_loss': train_loss/(last_idx+1)\n",
    "    #})\n",
    "\n",
    "\n",
    "def test(epoch, testloader, net, criterion, best_acc):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            #progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #             % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            if batch_idx % args.print_freq == 0:\n",
    "                print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "                             % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    #wandb.log({\n",
    "    #    'test_acc': 100.*correct/total,\n",
    "    #    'test_loss': test_loss/(batch_idx+1)\n",
    "    #})\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        #print('Saving..')\n",
    "        #state = {\n",
    "        #    'net': net.state_dict(),\n",
    "        #    'acc': acc,\n",
    "        #    'epoch': epoch,\n",
    "        #}\n",
    "        #save_path = os.path.join(args.save_dir, args.block)\n",
    "        #if not os.path.exists(save_path):\n",
    "        #    os.makedirs(save_path)\n",
    "        #torch.save(state, os.path.join(save_path, 'checkpoint_{}.pth'.format(epoch)))\n",
    "        best_acc = acc\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1234fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(net):\n",
    "    best_acc = 0  # best test accuracy\n",
    "    start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "    print(\"model : \", net)\n",
    "    net = net.to(device)\n",
    "    if device == 'cuda':\n",
    "        net = torch.nn.DataParallel(net)\n",
    "        # cudnn.benchmark = True\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=args.lr,\n",
    "                          momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "    test_accuracy = []\n",
    "    for idx in range(start_epoch, start_epoch+args.epochs):\n",
    "        train(idx, trainloader, net, criterion, optimizer)\n",
    "        acc = test(idx, testloader, net, criterion, best_acc)\n",
    "        scheduler.step()\n",
    "        test_accuracy.append(acc)\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbfd34b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model :  ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
      ")\n",
      "\n",
      "Epoch: 0\n",
      "0 391 Loss: 4.775 | Acc: 2.344% (3/128)\n",
      "100 391 Loss: 4.494 | Acc: 4.069% (526/12928)\n",
      "200 391 Loss: 4.255 | Acc: 5.613% (1444/25728)\n",
      "300 391 Loss: 4.124 | Acc: 6.855% (2641/38528)\n",
      "0 100 Loss: 3.886 | Acc: 9.000% (9/100)\n",
      "\n",
      "Epoch: 1\n",
      "0 391 Loss: 3.618 | Acc: 10.156% (13/128)\n",
      "100 391 Loss: 3.614 | Acc: 12.949% (1674/12928)\n",
      "200 391 Loss: 3.534 | Acc: 14.509% (3733/25728)\n",
      "300 391 Loss: 3.479 | Acc: 15.607% (6013/38528)\n",
      "0 100 Loss: 3.534 | Acc: 20.000% (20/100)\n",
      "\n",
      "Epoch: 2\n",
      "0 391 Loss: 3.219 | Acc: 19.531% (25/128)\n",
      "100 391 Loss: 3.130 | Acc: 22.486% (2907/12928)\n",
      "200 391 Loss: 3.086 | Acc: 23.329% (6002/25728)\n",
      "300 391 Loss: 3.041 | Acc: 24.079% (9277/38528)\n",
      "0 100 Loss: 3.060 | Acc: 25.000% (25/100)\n",
      "\n",
      "Epoch: 3\n",
      "0 391 Loss: 2.893 | Acc: 32.812% (42/128)\n",
      "100 391 Loss: 2.719 | Acc: 30.299% (3917/12928)\n",
      "200 391 Loss: 2.675 | Acc: 30.892% (7948/25728)\n",
      "300 391 Loss: 2.637 | Acc: 31.725% (12223/38528)\n",
      "0 100 Loss: 2.527 | Acc: 36.000% (36/100)\n",
      "\n",
      "Epoch: 4\n",
      "0 391 Loss: 2.298 | Acc: 37.500% (48/128)\n",
      "100 391 Loss: 2.276 | Acc: 39.024% (5045/12928)\n",
      "200 391 Loss: 2.262 | Acc: 39.521% (10168/25728)\n",
      "300 391 Loss: 2.239 | Acc: 39.924% (15382/38528)\n",
      "0 100 Loss: 2.083 | Acc: 41.000% (41/100)\n",
      "\n",
      "Epoch: 5\n",
      "0 391 Loss: 2.082 | Acc: 48.438% (62/128)\n",
      "100 391 Loss: 2.002 | Acc: 44.964% (5813/12928)\n",
      "200 391 Loss: 1.999 | Acc: 45.437% (11690/25728)\n",
      "300 391 Loss: 1.990 | Acc: 45.741% (17623/38528)\n",
      "0 100 Loss: 2.280 | Acc: 42.000% (42/100)\n",
      "\n",
      "Epoch: 6\n",
      "0 391 Loss: 1.936 | Acc: 42.188% (54/128)\n",
      "100 391 Loss: 1.805 | Acc: 49.876% (6448/12928)\n",
      "200 391 Loss: 1.811 | Acc: 49.767% (12804/25728)\n",
      "300 391 Loss: 1.807 | Acc: 50.023% (19273/38528)\n",
      "0 100 Loss: 1.894 | Acc: 50.000% (50/100)\n",
      "\n",
      "Epoch: 7\n",
      "0 391 Loss: 1.824 | Acc: 50.000% (64/128)\n",
      "100 391 Loss: 1.670 | Acc: 53.187% (6876/12928)\n",
      "200 391 Loss: 1.678 | Acc: 52.989% (13633/25728)\n",
      "300 391 Loss: 1.681 | Acc: 52.925% (20391/38528)\n",
      "0 100 Loss: 1.984 | Acc: 53.000% (53/100)\n",
      "\n",
      "Epoch: 8\n",
      "0 391 Loss: 1.756 | Acc: 51.562% (66/128)\n",
      "100 391 Loss: 1.577 | Acc: 55.469% (7171/12928)\n",
      "200 391 Loss: 1.578 | Acc: 55.422% (14259/25728)\n",
      "300 391 Loss: 1.583 | Acc: 55.406% (21347/38528)\n",
      "0 100 Loss: 1.948 | Acc: 54.000% (54/100)\n",
      "\n",
      "Epoch: 9\n",
      "0 391 Loss: 1.565 | Acc: 55.469% (71/128)\n",
      "100 391 Loss: 1.466 | Acc: 58.555% (7570/12928)\n",
      "200 391 Loss: 1.504 | Acc: 57.537% (14803/25728)\n",
      "300 391 Loss: 1.526 | Acc: 56.738% (21860/38528)\n",
      "0 100 Loss: 2.053 | Acc: 50.000% (50/100)\n",
      "\n",
      "Epoch: 10\n",
      "0 391 Loss: 1.622 | Acc: 50.781% (65/128)\n",
      "100 391 Loss: 1.424 | Acc: 59.452% (7686/12928)\n",
      "200 391 Loss: 1.441 | Acc: 58.924% (15160/25728)\n",
      "300 391 Loss: 1.460 | Acc: 58.474% (22529/38528)\n",
      "0 100 Loss: 1.939 | Acc: 54.000% (54/100)\n",
      "\n",
      "Epoch: 11\n",
      "0 391 Loss: 1.432 | Acc: 60.156% (77/128)\n",
      "100 391 Loss: 1.365 | Acc: 60.930% (7877/12928)\n",
      "200 391 Loss: 1.400 | Acc: 60.047% (15449/25728)\n",
      "300 391 Loss: 1.412 | Acc: 59.897% (23077/38528)\n",
      "0 100 Loss: 1.558 | Acc: 57.000% (57/100)\n",
      "\n",
      "Epoch: 12\n",
      "0 391 Loss: 1.389 | Acc: 56.250% (72/128)\n",
      "100 391 Loss: 1.323 | Acc: 61.974% (8012/12928)\n",
      "200 391 Loss: 1.361 | Acc: 60.844% (15654/25728)\n",
      "300 391 Loss: 1.370 | Acc: 60.748% (23405/38528)\n",
      "0 100 Loss: 1.712 | Acc: 52.000% (52/100)\n",
      "\n",
      "Epoch: 13\n",
      "0 391 Loss: 1.123 | Acc: 70.312% (90/128)\n",
      "100 391 Loss: 1.278 | Acc: 62.531% (8084/12928)\n",
      "200 391 Loss: 1.308 | Acc: 62.006% (15953/25728)\n",
      "300 391 Loss: 1.324 | Acc: 61.610% (23737/38528)\n",
      "0 100 Loss: 1.583 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 14\n",
      "0 391 Loss: 1.435 | Acc: 58.594% (75/128)\n",
      "100 391 Loss: 1.268 | Acc: 62.925% (8135/12928)\n",
      "200 391 Loss: 1.291 | Acc: 62.414% (16058/25728)\n",
      "300 391 Loss: 1.305 | Acc: 62.131% (23938/38528)\n",
      "0 100 Loss: 1.779 | Acc: 55.000% (55/100)\n",
      "\n",
      "Epoch: 15\n",
      "0 391 Loss: 1.126 | Acc: 67.188% (86/128)\n",
      "100 391 Loss: 1.252 | Acc: 63.560% (8217/12928)\n",
      "200 391 Loss: 1.262 | Acc: 63.666% (16380/25728)\n",
      "300 391 Loss: 1.277 | Acc: 63.401% (24427/38528)\n",
      "0 100 Loss: 1.959 | Acc: 50.000% (50/100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 16\n",
      "0 391 Loss: 1.167 | Acc: 67.969% (87/128)\n",
      "100 391 Loss: 1.224 | Acc: 64.109% (8288/12928)\n",
      "200 391 Loss: 1.248 | Acc: 63.565% (16354/25728)\n",
      "300 391 Loss: 1.259 | Acc: 63.325% (24398/38528)\n",
      "0 100 Loss: 1.584 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 17\n",
      "0 391 Loss: 0.982 | Acc: 66.406% (85/128)\n",
      "100 391 Loss: 1.149 | Acc: 66.654% (8617/12928)\n",
      "200 391 Loss: 1.194 | Acc: 65.221% (16780/25728)\n",
      "300 391 Loss: 1.226 | Acc: 64.397% (24811/38528)\n",
      "0 100 Loss: 1.514 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 18\n",
      "0 391 Loss: 1.290 | Acc: 63.281% (81/128)\n",
      "100 391 Loss: 1.168 | Acc: 66.244% (8564/12928)\n",
      "200 391 Loss: 1.183 | Acc: 65.730% (16911/25728)\n",
      "300 391 Loss: 1.203 | Acc: 65.129% (25093/38528)\n",
      "0 100 Loss: 1.582 | Acc: 60.000% (60/100)\n",
      "\n",
      "Epoch: 19\n",
      "0 391 Loss: 1.148 | Acc: 67.969% (87/128)\n",
      "100 391 Loss: 1.142 | Acc: 66.801% (8636/12928)\n",
      "200 391 Loss: 1.171 | Acc: 65.986% (16977/25728)\n",
      "300 391 Loss: 1.193 | Acc: 65.547% (25254/38528)\n",
      "0 100 Loss: 1.935 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 20\n",
      "0 391 Loss: 1.328 | Acc: 61.719% (79/128)\n",
      "100 391 Loss: 1.092 | Acc: 68.301% (8830/12928)\n",
      "200 391 Loss: 1.133 | Acc: 67.086% (17260/25728)\n",
      "300 391 Loss: 1.162 | Acc: 66.092% (25464/38528)\n",
      "0 100 Loss: 1.737 | Acc: 55.000% (55/100)\n",
      "\n",
      "Epoch: 21\n",
      "0 391 Loss: 1.073 | Acc: 71.094% (91/128)\n",
      "100 391 Loss: 1.142 | Acc: 66.778% (8633/12928)\n",
      "200 391 Loss: 1.156 | Acc: 66.476% (17103/25728)\n",
      "300 391 Loss: 1.159 | Acc: 66.328% (25555/38528)\n",
      "0 100 Loss: 1.477 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 22\n",
      "0 391 Loss: 0.921 | Acc: 71.875% (92/128)\n",
      "100 391 Loss: 1.043 | Acc: 69.338% (8964/12928)\n",
      "200 391 Loss: 1.116 | Acc: 67.510% (17369/25728)\n",
      "300 391 Loss: 1.138 | Acc: 67.042% (25830/38528)\n",
      "0 100 Loss: 2.260 | Acc: 49.000% (49/100)\n",
      "\n",
      "Epoch: 23\n",
      "0 391 Loss: 1.166 | Acc: 64.844% (83/128)\n",
      "100 391 Loss: 1.085 | Acc: 68.789% (8893/12928)\n",
      "200 391 Loss: 1.114 | Acc: 67.576% (17386/25728)\n",
      "300 391 Loss: 1.131 | Acc: 67.040% (25829/38528)\n",
      "0 100 Loss: 1.626 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 24\n",
      "0 391 Loss: 0.936 | Acc: 73.438% (94/128)\n",
      "100 391 Loss: 1.077 | Acc: 68.093% (8803/12928)\n",
      "200 391 Loss: 1.097 | Acc: 67.650% (17405/25728)\n",
      "300 391 Loss: 1.114 | Acc: 67.328% (25940/38528)\n",
      "0 100 Loss: 1.430 | Acc: 59.000% (59/100)\n",
      "\n",
      "Epoch: 25\n",
      "0 391 Loss: 0.929 | Acc: 71.094% (91/128)\n",
      "100 391 Loss: 1.065 | Acc: 68.464% (8851/12928)\n",
      "200 391 Loss: 1.084 | Acc: 68.159% (17536/25728)\n",
      "300 391 Loss: 1.100 | Acc: 67.753% (26104/38528)\n",
      "0 100 Loss: 1.268 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 26\n",
      "0 391 Loss: 0.782 | Acc: 75.000% (96/128)\n",
      "100 391 Loss: 1.072 | Acc: 68.688% (8880/12928)\n",
      "200 391 Loss: 1.088 | Acc: 68.373% (17591/25728)\n",
      "300 391 Loss: 1.099 | Acc: 68.197% (26275/38528)\n",
      "0 100 Loss: 1.816 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 27\n",
      "0 391 Loss: 1.091 | Acc: 68.750% (88/128)\n",
      "100 391 Loss: 1.027 | Acc: 70.042% (9055/12928)\n",
      "200 391 Loss: 1.057 | Acc: 69.143% (17789/25728)\n",
      "300 391 Loss: 1.076 | Acc: 68.812% (26512/38528)\n",
      "0 100 Loss: 1.483 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 28\n",
      "0 391 Loss: 0.920 | Acc: 75.000% (96/128)\n",
      "100 391 Loss: 1.005 | Acc: 70.452% (9108/12928)\n",
      "200 391 Loss: 1.034 | Acc: 69.900% (17984/25728)\n",
      "300 391 Loss: 1.070 | Acc: 68.817% (26514/38528)\n",
      "0 100 Loss: 1.869 | Acc: 50.000% (50/100)\n",
      "\n",
      "Epoch: 29\n",
      "0 391 Loss: 1.014 | Acc: 70.312% (90/128)\n",
      "100 391 Loss: 1.003 | Acc: 70.351% (9095/12928)\n",
      "200 391 Loss: 1.036 | Acc: 69.698% (17932/25728)\n",
      "300 391 Loss: 1.064 | Acc: 69.126% (26633/38528)\n",
      "0 100 Loss: 1.420 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 30\n",
      "0 391 Loss: 0.939 | Acc: 69.531% (89/128)\n",
      "100 391 Loss: 0.997 | Acc: 70.715% (9142/12928)\n",
      "200 391 Loss: 1.048 | Acc: 69.073% (17771/25728)\n",
      "300 391 Loss: 1.057 | Acc: 68.914% (26551/38528)\n",
      "0 100 Loss: 1.448 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 31\n",
      "0 391 Loss: 0.753 | Acc: 78.125% (100/128)\n",
      "100 391 Loss: 0.987 | Acc: 70.947% (9172/12928)\n",
      "200 391 Loss: 1.030 | Acc: 69.733% (17941/25728)\n",
      "300 391 Loss: 1.048 | Acc: 69.264% (26686/38528)\n",
      "0 100 Loss: 1.455 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 32\n",
      "0 391 Loss: 1.032 | Acc: 68.750% (88/128)\n",
      "100 391 Loss: 0.998 | Acc: 70.289% (9087/12928)\n",
      "200 391 Loss: 1.017 | Acc: 70.110% (18038/25728)\n",
      "300 391 Loss: 1.047 | Acc: 69.316% (26706/38528)\n",
      "0 100 Loss: 1.503 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 33\n",
      "0 391 Loss: 1.092 | Acc: 67.188% (86/128)\n",
      "100 391 Loss: 0.954 | Acc: 71.782% (9280/12928)\n",
      "200 391 Loss: 0.996 | Acc: 70.655% (18178/25728)\n",
      "300 391 Loss: 1.023 | Acc: 69.900% (26931/38528)\n",
      "0 100 Loss: 1.667 | Acc: 60.000% (60/100)\n",
      "\n",
      "Epoch: 34\n",
      "0 391 Loss: 1.211 | Acc: 62.500% (80/128)\n",
      "100 391 Loss: 0.939 | Acc: 72.401% (9360/12928)\n",
      "200 391 Loss: 0.983 | Acc: 71.090% (18290/25728)\n",
      "300 391 Loss: 1.016 | Acc: 70.209% (27050/38528)\n",
      "0 100 Loss: 1.677 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 35\n",
      "0 391 Loss: 0.793 | Acc: 79.688% (102/128)\n",
      "100 391 Loss: 0.926 | Acc: 72.548% (9379/12928)\n",
      "200 391 Loss: 0.976 | Acc: 71.280% (18339/25728)\n",
      "300 391 Loss: 1.010 | Acc: 70.492% (27159/38528)\n",
      "0 100 Loss: 1.611 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 36\n",
      "0 391 Loss: 0.868 | Acc: 70.312% (90/128)\n",
      "100 391 Loss: 0.958 | Acc: 71.883% (9293/12928)\n",
      "200 391 Loss: 0.994 | Acc: 70.899% (18241/25728)\n",
      "300 391 Loss: 1.015 | Acc: 70.258% (27069/38528)\n",
      "0 100 Loss: 1.315 | Acc: 59.000% (59/100)\n",
      "\n",
      "Epoch: 37\n",
      "0 391 Loss: 0.715 | Acc: 80.469% (103/128)\n",
      "100 391 Loss: 0.927 | Acc: 72.123% (9324/12928)\n",
      "200 391 Loss: 0.971 | Acc: 71.000% (18267/25728)\n",
      "300 391 Loss: 0.992 | Acc: 70.468% (27150/38528)\n",
      "0 100 Loss: 1.713 | Acc: 51.000% (51/100)\n",
      "\n",
      "Epoch: 38\n",
      "0 391 Loss: 0.994 | Acc: 71.094% (91/128)\n",
      "100 391 Loss: 0.958 | Acc: 71.349% (9224/12928)\n",
      "200 391 Loss: 0.967 | Acc: 71.308% (18346/25728)\n",
      "300 391 Loss: 0.991 | Acc: 70.790% (27274/38528)\n",
      "0 100 Loss: 1.405 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 39\n",
      "0 391 Loss: 1.007 | Acc: 69.531% (89/128)\n",
      "100 391 Loss: 0.924 | Acc: 72.780% (9409/12928)\n",
      "200 391 Loss: 0.963 | Acc: 71.642% (18432/25728)\n",
      "300 391 Loss: 0.987 | Acc: 71.052% (27375/38528)\n",
      "0 100 Loss: 1.516 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 40\n",
      "0 391 Loss: 0.931 | Acc: 70.312% (90/128)\n",
      "100 391 Loss: 0.909 | Acc: 72.958% (9432/12928)\n",
      "200 391 Loss: 0.943 | Acc: 71.992% (18522/25728)\n",
      "300 391 Loss: 0.970 | Acc: 71.242% (27448/38528)\n",
      "0 100 Loss: 1.538 | Acc: 57.000% (57/100)\n",
      "\n",
      "Epoch: 41\n",
      "0 391 Loss: 0.897 | Acc: 71.875% (92/128)\n",
      "100 391 Loss: 0.905 | Acc: 73.144% (9456/12928)\n",
      "200 391 Loss: 0.953 | Acc: 71.786% (18469/25728)\n",
      "300 391 Loss: 0.969 | Acc: 71.278% (27462/38528)\n",
      "0 100 Loss: 1.271 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 42\n",
      "0 391 Loss: 0.869 | Acc: 71.094% (91/128)\n",
      "100 391 Loss: 0.880 | Acc: 73.654% (9522/12928)\n",
      "200 391 Loss: 0.926 | Acc: 72.485% (18649/25728)\n",
      "300 391 Loss: 0.955 | Acc: 71.797% (27662/38528)\n",
      "0 100 Loss: 1.571 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 43\n",
      "0 391 Loss: 0.757 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.907 | Acc: 73.267% (9472/12928)\n",
      "200 391 Loss: 0.928 | Acc: 72.769% (18722/25728)\n",
      "300 391 Loss: 0.956 | Acc: 71.932% (27714/38528)\n",
      "0 100 Loss: 1.263 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 44\n",
      "0 391 Loss: 0.893 | Acc: 71.875% (92/128)\n",
      "100 391 Loss: 0.885 | Acc: 73.523% (9505/12928)\n",
      "200 391 Loss: 0.917 | Acc: 72.893% (18754/25728)\n",
      "300 391 Loss: 0.936 | Acc: 72.246% (27835/38528)\n",
      "0 100 Loss: 1.689 | Acc: 59.000% (59/100)\n",
      "\n",
      "Epoch: 45\n",
      "0 391 Loss: 1.011 | Acc: 67.188% (86/128)\n",
      "100 391 Loss: 0.879 | Acc: 73.987% (9565/12928)\n",
      "200 391 Loss: 0.909 | Acc: 73.095% (18806/25728)\n",
      "300 391 Loss: 0.932 | Acc: 72.376% (27885/38528)\n",
      "0 100 Loss: 1.501 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 46\n",
      "0 391 Loss: 0.831 | Acc: 72.656% (93/128)\n",
      "100 391 Loss: 0.863 | Acc: 74.250% (9599/12928)\n",
      "200 391 Loss: 0.908 | Acc: 72.862% (18746/25728)\n",
      "300 391 Loss: 0.937 | Acc: 72.148% (27797/38528)\n",
      "0 100 Loss: 1.747 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 47\n",
      "0 391 Loss: 0.804 | Acc: 76.562% (98/128)\n",
      "100 391 Loss: 0.858 | Acc: 73.832% (9545/12928)\n",
      "200 391 Loss: 0.894 | Acc: 73.301% (18859/25728)\n",
      "300 391 Loss: 0.920 | Acc: 72.750% (28029/38528)\n",
      "0 100 Loss: 1.797 | Acc: 55.000% (55/100)\n",
      "\n",
      "Epoch: 48\n",
      "0 391 Loss: 0.886 | Acc: 72.656% (93/128)\n",
      "100 391 Loss: 0.838 | Acc: 74.884% (9681/12928)\n",
      "200 391 Loss: 0.881 | Acc: 73.554% (18924/25728)\n",
      "300 391 Loss: 0.916 | Acc: 72.690% (28006/38528)\n",
      "0 100 Loss: 1.564 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 49\n",
      "0 391 Loss: 0.898 | Acc: 71.875% (92/128)\n",
      "100 391 Loss: 0.848 | Acc: 74.938% (9688/12928)\n",
      "200 391 Loss: 0.885 | Acc: 73.733% (18970/25728)\n",
      "300 391 Loss: 0.908 | Acc: 73.175% (28193/38528)\n",
      "0 100 Loss: 1.468 | Acc: 57.000% (57/100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 50\n",
      "0 391 Loss: 0.801 | Acc: 74.219% (95/128)\n",
      "100 391 Loss: 0.850 | Acc: 74.466% (9627/12928)\n",
      "200 391 Loss: 0.879 | Acc: 73.884% (19009/25728)\n",
      "300 391 Loss: 0.906 | Acc: 73.152% (28184/38528)\n",
      "0 100 Loss: 1.444 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 51\n",
      "0 391 Loss: 0.901 | Acc: 71.875% (92/128)\n",
      "100 391 Loss: 0.808 | Acc: 76.006% (9826/12928)\n",
      "200 391 Loss: 0.852 | Acc: 74.677% (19213/25728)\n",
      "300 391 Loss: 0.888 | Acc: 73.720% (28403/38528)\n",
      "0 100 Loss: 1.391 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 52\n",
      "0 391 Loss: 0.569 | Acc: 82.031% (105/128)\n",
      "100 391 Loss: 0.842 | Acc: 74.892% (9682/12928)\n",
      "200 391 Loss: 0.860 | Acc: 74.141% (19075/25728)\n",
      "300 391 Loss: 0.892 | Acc: 73.370% (28268/38528)\n",
      "0 100 Loss: 1.311 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 53\n",
      "0 391 Loss: 0.993 | Acc: 73.438% (94/128)\n",
      "100 391 Loss: 0.832 | Acc: 75.054% (9703/12928)\n",
      "200 391 Loss: 0.864 | Acc: 73.896% (19012/25728)\n",
      "300 391 Loss: 0.887 | Acc: 73.380% (28272/38528)\n",
      "0 100 Loss: 1.401 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 54\n",
      "0 391 Loss: 0.699 | Acc: 76.562% (98/128)\n",
      "100 391 Loss: 0.837 | Acc: 74.830% (9674/12928)\n",
      "200 391 Loss: 0.861 | Acc: 74.242% (19101/25728)\n",
      "300 391 Loss: 0.886 | Acc: 73.661% (28380/38528)\n",
      "0 100 Loss: 1.693 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 55\n",
      "0 391 Loss: 0.863 | Acc: 75.781% (97/128)\n",
      "100 391 Loss: 0.822 | Acc: 75.495% (9760/12928)\n",
      "200 391 Loss: 0.846 | Acc: 74.860% (19260/25728)\n",
      "300 391 Loss: 0.872 | Acc: 74.128% (28560/38528)\n",
      "0 100 Loss: 1.607 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 56\n",
      "0 391 Loss: 0.760 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.804 | Acc: 76.029% (9829/12928)\n",
      "200 391 Loss: 0.837 | Acc: 75.229% (19355/25728)\n",
      "300 391 Loss: 0.858 | Acc: 74.507% (28706/38528)\n",
      "0 100 Loss: 1.483 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 57\n",
      "0 391 Loss: 0.629 | Acc: 82.812% (106/128)\n",
      "100 391 Loss: 0.805 | Acc: 75.982% (9823/12928)\n",
      "200 391 Loss: 0.837 | Acc: 74.716% (19223/25728)\n",
      "300 391 Loss: 0.861 | Acc: 74.060% (28534/38528)\n",
      "0 100 Loss: 1.509 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 58\n",
      "0 391 Loss: 0.682 | Acc: 78.906% (101/128)\n",
      "100 391 Loss: 0.800 | Acc: 76.199% (9851/12928)\n",
      "200 391 Loss: 0.809 | Acc: 75.820% (19507/25728)\n",
      "300 391 Loss: 0.840 | Acc: 74.945% (28875/38528)\n",
      "0 100 Loss: 1.790 | Acc: 53.000% (53/100)\n",
      "\n",
      "Epoch: 59\n",
      "0 391 Loss: 0.648 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.802 | Acc: 75.874% (9809/12928)\n",
      "200 391 Loss: 0.820 | Acc: 75.521% (19430/25728)\n",
      "300 391 Loss: 0.844 | Acc: 74.943% (28874/38528)\n",
      "0 100 Loss: 1.401 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 60\n",
      "0 391 Loss: 0.844 | Acc: 80.469% (103/128)\n",
      "100 391 Loss: 0.775 | Acc: 76.849% (9935/12928)\n",
      "200 391 Loss: 0.804 | Acc: 75.723% (19482/25728)\n",
      "300 391 Loss: 0.833 | Acc: 74.951% (28877/38528)\n",
      "0 100 Loss: 1.724 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 61\n",
      "0 391 Loss: 0.932 | Acc: 69.531% (89/128)\n",
      "100 391 Loss: 0.773 | Acc: 76.694% (9915/12928)\n",
      "200 391 Loss: 0.815 | Acc: 75.731% (19484/25728)\n",
      "300 391 Loss: 0.835 | Acc: 75.169% (28961/38528)\n",
      "0 100 Loss: 1.806 | Acc: 59.000% (59/100)\n",
      "\n",
      "Epoch: 62\n",
      "0 391 Loss: 0.847 | Acc: 68.750% (88/128)\n",
      "100 391 Loss: 0.757 | Acc: 77.150% (9974/12928)\n",
      "200 391 Loss: 0.793 | Acc: 76.197% (19604/25728)\n",
      "300 391 Loss: 0.819 | Acc: 75.405% (29052/38528)\n",
      "0 100 Loss: 1.475 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 63\n",
      "0 391 Loss: 0.928 | Acc: 72.656% (93/128)\n",
      "100 391 Loss: 0.748 | Acc: 77.630% (10036/12928)\n",
      "200 391 Loss: 0.773 | Acc: 77.017% (19815/25728)\n",
      "300 391 Loss: 0.804 | Acc: 76.002% (29282/38528)\n",
      "0 100 Loss: 1.435 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 64\n",
      "0 391 Loss: 0.721 | Acc: 82.031% (105/128)\n",
      "100 391 Loss: 0.756 | Acc: 77.181% (9978/12928)\n",
      "200 391 Loss: 0.784 | Acc: 76.302% (19631/25728)\n",
      "300 391 Loss: 0.810 | Acc: 75.600% (29127/38528)\n",
      "0 100 Loss: 1.269 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 65\n",
      "0 391 Loss: 0.861 | Acc: 76.562% (98/128)\n",
      "100 391 Loss: 0.711 | Acc: 78.303% (10123/12928)\n",
      "200 391 Loss: 0.749 | Acc: 77.258% (19877/25728)\n",
      "300 391 Loss: 0.793 | Acc: 76.059% (29304/38528)\n",
      "0 100 Loss: 1.540 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 66\n",
      "0 391 Loss: 0.680 | Acc: 79.688% (102/128)\n",
      "100 391 Loss: 0.717 | Acc: 78.241% (10115/12928)\n",
      "200 391 Loss: 0.760 | Acc: 77.309% (19890/25728)\n",
      "300 391 Loss: 0.779 | Acc: 76.666% (29538/38528)\n",
      "0 100 Loss: 1.358 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 67\n",
      "0 391 Loss: 0.624 | Acc: 83.594% (107/128)\n",
      "100 391 Loss: 0.723 | Acc: 78.171% (10106/12928)\n",
      "200 391 Loss: 0.759 | Acc: 77.111% (19839/25728)\n",
      "300 391 Loss: 0.792 | Acc: 76.228% (29369/38528)\n",
      "0 100 Loss: 1.273 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 68\n",
      "0 391 Loss: 0.869 | Acc: 75.781% (97/128)\n",
      "100 391 Loss: 0.701 | Acc: 78.968% (10209/12928)\n",
      "200 391 Loss: 0.744 | Acc: 77.662% (19981/25728)\n",
      "300 391 Loss: 0.767 | Acc: 76.980% (29659/38528)\n",
      "0 100 Loss: 1.586 | Acc: 60.000% (60/100)\n",
      "\n",
      "Epoch: 69\n",
      "0 391 Loss: 0.639 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.696 | Acc: 79.146% (10232/12928)\n",
      "200 391 Loss: 0.740 | Acc: 77.876% (20036/25728)\n",
      "300 391 Loss: 0.770 | Acc: 76.915% (29634/38528)\n",
      "0 100 Loss: 1.257 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 70\n",
      "0 391 Loss: 0.569 | Acc: 82.031% (105/128)\n",
      "100 391 Loss: 0.684 | Acc: 79.100% (10226/12928)\n",
      "200 391 Loss: 0.732 | Acc: 77.802% (20017/25728)\n",
      "300 391 Loss: 0.757 | Acc: 77.056% (29688/38528)\n",
      "0 100 Loss: 1.602 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 71\n",
      "0 391 Loss: 0.673 | Acc: 76.562% (98/128)\n",
      "100 391 Loss: 0.699 | Acc: 78.628% (10165/12928)\n",
      "200 391 Loss: 0.731 | Acc: 77.569% (19957/25728)\n",
      "300 391 Loss: 0.748 | Acc: 77.030% (29678/38528)\n",
      "0 100 Loss: 1.400 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 72\n",
      "0 391 Loss: 0.592 | Acc: 82.812% (106/128)\n",
      "100 391 Loss: 0.683 | Acc: 79.664% (10299/12928)\n",
      "200 391 Loss: 0.710 | Acc: 78.692% (20246/25728)\n",
      "300 391 Loss: 0.744 | Acc: 77.616% (29904/38528)\n",
      "0 100 Loss: 1.458 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 73\n",
      "0 391 Loss: 0.507 | Acc: 82.031% (105/128)\n",
      "100 391 Loss: 0.692 | Acc: 79.053% (10220/12928)\n",
      "200 391 Loss: 0.724 | Acc: 78.160% (20109/25728)\n",
      "300 391 Loss: 0.748 | Acc: 77.411% (29825/38528)\n",
      "0 100 Loss: 1.568 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 74\n",
      "0 391 Loss: 0.406 | Acc: 89.062% (114/128)\n",
      "100 391 Loss: 0.669 | Acc: 79.788% (10315/12928)\n",
      "200 391 Loss: 0.701 | Acc: 78.615% (20226/25728)\n",
      "300 391 Loss: 0.738 | Acc: 77.559% (29882/38528)\n",
      "0 100 Loss: 1.465 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 75\n",
      "0 391 Loss: 0.645 | Acc: 78.125% (100/128)\n",
      "100 391 Loss: 0.663 | Acc: 79.394% (10264/12928)\n",
      "200 391 Loss: 0.692 | Acc: 78.957% (20314/25728)\n",
      "300 391 Loss: 0.717 | Acc: 78.307% (30170/38528)\n",
      "0 100 Loss: 1.313 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 76\n",
      "0 391 Loss: 0.729 | Acc: 75.781% (97/128)\n",
      "100 391 Loss: 0.670 | Acc: 79.602% (10291/12928)\n",
      "200 391 Loss: 0.678 | Acc: 79.303% (20403/25728)\n",
      "300 391 Loss: 0.708 | Acc: 78.436% (30220/38528)\n",
      "0 100 Loss: 1.373 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 77\n",
      "0 391 Loss: 0.641 | Acc: 83.594% (107/128)\n",
      "100 391 Loss: 0.647 | Acc: 80.608% (10421/12928)\n",
      "200 391 Loss: 0.659 | Acc: 80.138% (20618/25728)\n",
      "300 391 Loss: 0.700 | Acc: 78.857% (30382/38528)\n",
      "0 100 Loss: 1.444 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 78\n",
      "0 391 Loss: 0.641 | Acc: 82.031% (105/128)\n",
      "100 391 Loss: 0.642 | Acc: 80.654% (10427/12928)\n",
      "200 391 Loss: 0.672 | Acc: 79.602% (20480/25728)\n",
      "300 391 Loss: 0.700 | Acc: 78.771% (30349/38528)\n",
      "0 100 Loss: 1.397 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 79\n",
      "0 391 Loss: 0.662 | Acc: 79.688% (102/128)\n",
      "100 391 Loss: 0.640 | Acc: 80.097% (10355/12928)\n",
      "200 391 Loss: 0.660 | Acc: 79.769% (20523/25728)\n",
      "300 391 Loss: 0.694 | Acc: 78.797% (30359/38528)\n",
      "0 100 Loss: 1.607 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 80\n",
      "0 391 Loss: 0.787 | Acc: 78.906% (101/128)\n",
      "100 391 Loss: 0.623 | Acc: 81.428% (10527/12928)\n",
      "200 391 Loss: 0.646 | Acc: 80.558% (20726/25728)\n",
      "300 391 Loss: 0.670 | Acc: 79.734% (30720/38528)\n",
      "0 100 Loss: 1.781 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 81\n",
      "0 391 Loss: 0.446 | Acc: 88.281% (113/128)\n",
      "100 391 Loss: 0.632 | Acc: 80.631% (10424/12928)\n",
      "200 391 Loss: 0.655 | Acc: 79.983% (20578/25728)\n",
      "300 391 Loss: 0.677 | Acc: 79.376% (30582/38528)\n",
      "0 100 Loss: 1.692 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 82\n",
      "0 391 Loss: 0.576 | Acc: 83.594% (107/128)\n",
      "100 391 Loss: 0.626 | Acc: 80.368% (10390/12928)\n",
      "200 391 Loss: 0.647 | Acc: 80.045% (20594/25728)\n",
      "300 391 Loss: 0.679 | Acc: 79.233% (30527/38528)\n",
      "0 100 Loss: 1.473 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 83\n",
      "0 391 Loss: 0.590 | Acc: 82.031% (105/128)\n",
      "100 391 Loss: 0.594 | Acc: 82.248% (10633/12928)\n",
      "200 391 Loss: 0.622 | Acc: 81.083% (20861/25728)\n",
      "300 391 Loss: 0.648 | Acc: 80.310% (30942/38528)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100 Loss: 1.846 | Acc: 60.000% (60/100)\n",
      "\n",
      "Epoch: 84\n",
      "0 391 Loss: 0.590 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.602 | Acc: 81.521% (10539/12928)\n",
      "200 391 Loss: 0.616 | Acc: 81.017% (20844/25728)\n",
      "300 391 Loss: 0.638 | Acc: 80.378% (30968/38528)\n",
      "0 100 Loss: 1.499 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 85\n",
      "0 391 Loss: 0.527 | Acc: 83.594% (107/128)\n",
      "100 391 Loss: 0.584 | Acc: 82.024% (10604/12928)\n",
      "200 391 Loss: 0.616 | Acc: 80.927% (20821/25728)\n",
      "300 391 Loss: 0.642 | Acc: 80.214% (30905/38528)\n",
      "0 100 Loss: 1.434 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 86\n",
      "0 391 Loss: 0.623 | Acc: 80.469% (103/128)\n",
      "100 391 Loss: 0.565 | Acc: 82.890% (10716/12928)\n",
      "200 391 Loss: 0.598 | Acc: 81.635% (21003/25728)\n",
      "300 391 Loss: 0.626 | Acc: 80.746% (31110/38528)\n",
      "0 100 Loss: 1.244 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 87\n",
      "0 391 Loss: 0.724 | Acc: 78.906% (101/128)\n",
      "100 391 Loss: 0.571 | Acc: 82.751% (10698/12928)\n",
      "200 391 Loss: 0.586 | Acc: 82.311% (21177/25728)\n",
      "300 391 Loss: 0.619 | Acc: 81.310% (31327/38528)\n",
      "0 100 Loss: 1.316 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 88\n",
      "0 391 Loss: 0.643 | Acc: 82.812% (106/128)\n",
      "100 391 Loss: 0.552 | Acc: 82.828% (10708/12928)\n",
      "200 391 Loss: 0.590 | Acc: 81.856% (21060/25728)\n",
      "300 391 Loss: 0.612 | Acc: 81.190% (31281/38528)\n",
      "0 100 Loss: 1.547 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 89\n",
      "0 391 Loss: 0.614 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.549 | Acc: 83.021% (10733/12928)\n",
      "200 391 Loss: 0.571 | Acc: 82.377% (21194/25728)\n",
      "300 391 Loss: 0.601 | Acc: 81.484% (31394/38528)\n",
      "0 100 Loss: 1.393 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 90\n",
      "0 391 Loss: 0.459 | Acc: 88.281% (113/128)\n",
      "100 391 Loss: 0.561 | Acc: 82.727% (10695/12928)\n",
      "200 391 Loss: 0.573 | Acc: 82.521% (21231/25728)\n",
      "300 391 Loss: 0.591 | Acc: 81.940% (31570/38528)\n",
      "0 100 Loss: 1.301 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 91\n",
      "0 391 Loss: 0.611 | Acc: 82.031% (105/128)\n",
      "100 391 Loss: 0.519 | Acc: 84.097% (10872/12928)\n",
      "200 391 Loss: 0.547 | Acc: 83.053% (21368/25728)\n",
      "300 391 Loss: 0.573 | Acc: 82.210% (31674/38528)\n",
      "0 100 Loss: 1.891 | Acc: 57.000% (57/100)\n",
      "\n",
      "Epoch: 92\n",
      "0 391 Loss: 0.721 | Acc: 73.438% (94/128)\n",
      "100 391 Loss: 0.522 | Acc: 84.298% (10898/12928)\n",
      "200 391 Loss: 0.532 | Acc: 83.928% (21593/25728)\n",
      "300 391 Loss: 0.563 | Acc: 82.831% (31913/38528)\n",
      "0 100 Loss: 1.987 | Acc: 54.000% (54/100)\n",
      "\n",
      "Epoch: 93\n",
      "0 391 Loss: 0.672 | Acc: 79.688% (102/128)\n",
      "100 391 Loss: 0.516 | Acc: 83.988% (10858/12928)\n",
      "200 391 Loss: 0.544 | Acc: 83.221% (21411/25728)\n",
      "300 391 Loss: 0.558 | Acc: 83.064% (32003/38528)\n",
      "0 100 Loss: 1.472 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 94\n",
      "0 391 Loss: 0.538 | Acc: 82.812% (106/128)\n",
      "100 391 Loss: 0.506 | Acc: 84.491% (10923/12928)\n",
      "200 391 Loss: 0.518 | Acc: 84.014% (21615/25728)\n",
      "300 391 Loss: 0.550 | Acc: 83.031% (31990/38528)\n",
      "0 100 Loss: 1.580 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 95\n",
      "0 391 Loss: 0.507 | Acc: 83.594% (107/128)\n",
      "100 391 Loss: 0.500 | Acc: 84.978% (10986/12928)\n",
      "200 391 Loss: 0.517 | Acc: 84.192% (21661/25728)\n",
      "300 391 Loss: 0.538 | Acc: 83.467% (32158/38528)\n",
      "0 100 Loss: 1.400 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 96\n",
      "0 391 Loss: 0.584 | Acc: 79.688% (102/128)\n",
      "100 391 Loss: 0.479 | Acc: 85.056% (10996/12928)\n",
      "200 391 Loss: 0.497 | Acc: 84.736% (21801/25728)\n",
      "300 391 Loss: 0.527 | Acc: 83.833% (32299/38528)\n",
      "0 100 Loss: 1.302 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 97\n",
      "0 391 Loss: 0.477 | Acc: 85.156% (109/128)\n",
      "100 391 Loss: 0.472 | Acc: 85.396% (11040/12928)\n",
      "200 391 Loss: 0.496 | Acc: 84.581% (21761/25728)\n",
      "300 391 Loss: 0.524 | Acc: 83.783% (32280/38528)\n",
      "0 100 Loss: 1.468 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 98\n",
      "0 391 Loss: 0.615 | Acc: 82.031% (105/128)\n",
      "100 391 Loss: 0.487 | Acc: 85.156% (11009/12928)\n",
      "200 391 Loss: 0.488 | Acc: 84.919% (21848/25728)\n",
      "300 391 Loss: 0.517 | Acc: 84.154% (32423/38528)\n",
      "0 100 Loss: 1.282 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 99\n",
      "0 391 Loss: 0.624 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.453 | Acc: 86.340% (11162/12928)\n",
      "200 391 Loss: 0.474 | Acc: 85.428% (21979/25728)\n",
      "300 391 Loss: 0.501 | Acc: 84.536% (32570/38528)\n",
      "0 100 Loss: 1.180 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 100\n",
      "0 391 Loss: 0.582 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.447 | Acc: 85.837% (11097/12928)\n",
      "200 391 Loss: 0.469 | Acc: 85.370% (21964/25728)\n",
      "300 391 Loss: 0.495 | Acc: 84.751% (32653/38528)\n",
      "0 100 Loss: 1.567 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 101\n",
      "0 391 Loss: 0.469 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.466 | Acc: 85.690% (11078/12928)\n",
      "200 391 Loss: 0.469 | Acc: 85.564% (22014/25728)\n",
      "300 391 Loss: 0.481 | Acc: 85.065% (32774/38528)\n",
      "0 100 Loss: 1.631 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 102\n",
      "0 391 Loss: 0.383 | Acc: 87.500% (112/128)\n",
      "100 391 Loss: 0.441 | Acc: 86.448% (11176/12928)\n",
      "200 391 Loss: 0.455 | Acc: 86.000% (22126/25728)\n",
      "300 391 Loss: 0.474 | Acc: 85.408% (32906/38528)\n",
      "0 100 Loss: 1.560 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 103\n",
      "0 391 Loss: 0.481 | Acc: 85.156% (109/128)\n",
      "100 391 Loss: 0.432 | Acc: 86.487% (11181/12928)\n",
      "200 391 Loss: 0.444 | Acc: 86.085% (22148/25728)\n",
      "300 391 Loss: 0.466 | Acc: 85.452% (32923/38528)\n",
      "0 100 Loss: 1.571 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 104\n",
      "0 391 Loss: 0.381 | Acc: 89.844% (115/128)\n",
      "100 391 Loss: 0.414 | Acc: 87.369% (11295/12928)\n",
      "200 391 Loss: 0.435 | Acc: 86.660% (22296/25728)\n",
      "300 391 Loss: 0.453 | Acc: 86.083% (33166/38528)\n",
      "0 100 Loss: 1.538 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 105\n",
      "0 391 Loss: 0.371 | Acc: 87.500% (112/128)\n",
      "100 391 Loss: 0.410 | Acc: 87.670% (11334/12928)\n",
      "200 391 Loss: 0.418 | Acc: 87.224% (22441/25728)\n",
      "300 391 Loss: 0.443 | Acc: 86.332% (33262/38528)\n",
      "0 100 Loss: 1.290 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 106\n",
      "0 391 Loss: 0.378 | Acc: 86.719% (111/128)\n",
      "100 391 Loss: 0.406 | Acc: 87.307% (11287/12928)\n",
      "200 391 Loss: 0.413 | Acc: 87.142% (22420/25728)\n",
      "300 391 Loss: 0.443 | Acc: 86.109% (33176/38528)\n",
      "0 100 Loss: 1.151 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 107\n",
      "0 391 Loss: 0.398 | Acc: 89.844% (115/128)\n",
      "100 391 Loss: 0.390 | Acc: 88.088% (11388/12928)\n",
      "200 391 Loss: 0.410 | Acc: 87.325% (22467/25728)\n",
      "300 391 Loss: 0.430 | Acc: 86.672% (33393/38528)\n",
      "0 100 Loss: 1.327 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 108\n",
      "0 391 Loss: 0.448 | Acc: 88.281% (113/128)\n",
      "100 391 Loss: 0.386 | Acc: 88.142% (11395/12928)\n",
      "200 391 Loss: 0.397 | Acc: 87.788% (22586/25728)\n",
      "300 391 Loss: 0.417 | Acc: 87.100% (33558/38528)\n",
      "0 100 Loss: 1.574 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 109\n",
      "0 391 Loss: 0.415 | Acc: 85.156% (109/128)\n",
      "100 391 Loss: 0.387 | Acc: 88.065% (11385/12928)\n",
      "200 391 Loss: 0.399 | Acc: 87.574% (22531/25728)\n",
      "300 391 Loss: 0.410 | Acc: 87.285% (33629/38528)\n",
      "0 100 Loss: 1.382 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 110\n",
      "0 391 Loss: 0.497 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.375 | Acc: 88.467% (11437/12928)\n",
      "200 391 Loss: 0.388 | Acc: 88.005% (22642/25728)\n",
      "300 391 Loss: 0.404 | Acc: 87.497% (33711/38528)\n",
      "0 100 Loss: 1.346 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 111\n",
      "0 391 Loss: 0.250 | Acc: 89.062% (114/128)\n",
      "100 391 Loss: 0.337 | Acc: 89.851% (11616/12928)\n",
      "200 391 Loss: 0.356 | Acc: 89.020% (22903/25728)\n",
      "300 391 Loss: 0.385 | Acc: 88.040% (33920/38528)\n",
      "0 100 Loss: 1.512 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 112\n",
      "0 391 Loss: 0.410 | Acc: 88.281% (113/128)\n",
      "100 391 Loss: 0.357 | Acc: 88.977% (11503/12928)\n",
      "200 391 Loss: 0.362 | Acc: 88.763% (22837/25728)\n",
      "300 391 Loss: 0.379 | Acc: 88.237% (33996/38528)\n",
      "0 100 Loss: 1.414 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 113\n",
      "0 391 Loss: 0.206 | Acc: 96.094% (123/128)\n",
      "100 391 Loss: 0.333 | Acc: 89.735% (11601/12928)\n",
      "200 391 Loss: 0.342 | Acc: 89.467% (23018/25728)\n",
      "300 391 Loss: 0.367 | Acc: 88.613% (34141/38528)\n",
      "0 100 Loss: 1.638 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 114\n",
      "0 391 Loss: 0.267 | Acc: 90.625% (116/128)\n",
      "100 391 Loss: 0.314 | Acc: 90.408% (11688/12928)\n",
      "200 391 Loss: 0.327 | Acc: 89.883% (23125/25728)\n",
      "300 391 Loss: 0.343 | Acc: 89.452% (34464/38528)\n",
      "0 100 Loss: 1.707 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 115\n",
      "0 391 Loss: 0.386 | Acc: 89.062% (114/128)\n",
      "100 391 Loss: 0.334 | Acc: 89.774% (11606/12928)\n",
      "200 391 Loss: 0.339 | Acc: 89.611% (23055/25728)\n",
      "300 391 Loss: 0.359 | Acc: 89.029% (34301/38528)\n",
      "0 100 Loss: 1.336 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 116\n",
      "0 391 Loss: 0.234 | Acc: 92.188% (118/128)\n",
      "100 391 Loss: 0.311 | Acc: 90.347% (11680/12928)\n",
      "200 391 Loss: 0.321 | Acc: 90.046% (23167/25728)\n",
      "300 391 Loss: 0.340 | Acc: 89.395% (34442/38528)\n",
      "0 100 Loss: 1.496 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 117\n",
      "0 391 Loss: 0.354 | Acc: 89.062% (114/128)\n",
      "100 391 Loss: 0.297 | Acc: 91.105% (11778/12928)\n",
      "200 391 Loss: 0.292 | Acc: 91.161% (23454/25728)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 391 Loss: 0.305 | Acc: 90.768% (34971/38528)\n",
      "0 100 Loss: 1.285 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 118\n",
      "0 391 Loss: 0.238 | Acc: 92.969% (119/128)\n",
      "100 391 Loss: 0.300 | Acc: 90.579% (11710/12928)\n",
      "200 391 Loss: 0.306 | Acc: 90.497% (23283/25728)\n",
      "300 391 Loss: 0.323 | Acc: 89.836% (34612/38528)\n",
      "0 100 Loss: 1.322 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 119\n",
      "0 391 Loss: 0.397 | Acc: 84.375% (108/128)\n",
      "100 391 Loss: 0.290 | Acc: 91.197% (11790/12928)\n",
      "200 391 Loss: 0.294 | Acc: 91.049% (23425/25728)\n",
      "300 391 Loss: 0.311 | Acc: 90.415% (34835/38528)\n",
      "0 100 Loss: 1.433 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 120\n",
      "0 391 Loss: 0.200 | Acc: 93.750% (120/128)\n",
      "100 391 Loss: 0.260 | Acc: 92.218% (11922/12928)\n",
      "200 391 Loss: 0.270 | Acc: 91.950% (23657/25728)\n",
      "300 391 Loss: 0.283 | Acc: 91.380% (35207/38528)\n",
      "0 100 Loss: 1.392 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 121\n",
      "0 391 Loss: 0.235 | Acc: 92.188% (118/128)\n",
      "100 391 Loss: 0.273 | Acc: 91.507% (11830/12928)\n",
      "200 391 Loss: 0.275 | Acc: 91.531% (23549/25728)\n",
      "300 391 Loss: 0.286 | Acc: 91.100% (35099/38528)\n",
      "0 100 Loss: 1.281 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 122\n",
      "0 391 Loss: 0.252 | Acc: 92.188% (118/128)\n",
      "100 391 Loss: 0.265 | Acc: 91.894% (11880/12928)\n",
      "200 391 Loss: 0.267 | Acc: 91.900% (23644/25728)\n",
      "300 391 Loss: 0.272 | Acc: 91.770% (35357/38528)\n",
      "0 100 Loss: 1.542 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 123\n",
      "0 391 Loss: 0.402 | Acc: 88.281% (113/128)\n",
      "100 391 Loss: 0.251 | Acc: 92.667% (11980/12928)\n",
      "200 391 Loss: 0.269 | Acc: 92.090% (23693/25728)\n",
      "300 391 Loss: 0.280 | Acc: 91.640% (35307/38528)\n",
      "0 100 Loss: 1.213 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 124\n",
      "0 391 Loss: 0.352 | Acc: 89.844% (115/128)\n",
      "100 391 Loss: 0.228 | Acc: 93.518% (12090/12928)\n",
      "200 391 Loss: 0.233 | Acc: 93.171% (23971/25728)\n",
      "300 391 Loss: 0.250 | Acc: 92.494% (35636/38528)\n",
      "0 100 Loss: 1.309 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 125\n",
      "0 391 Loss: 0.259 | Acc: 93.750% (120/128)\n",
      "100 391 Loss: 0.234 | Acc: 93.139% (12041/12928)\n",
      "200 391 Loss: 0.240 | Acc: 92.856% (23890/25728)\n",
      "300 391 Loss: 0.259 | Acc: 92.066% (35471/38528)\n",
      "0 100 Loss: 1.418 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 126\n",
      "0 391 Loss: 0.280 | Acc: 91.406% (117/128)\n",
      "100 391 Loss: 0.253 | Acc: 92.420% (11948/12928)\n",
      "200 391 Loss: 0.248 | Acc: 92.533% (23807/25728)\n",
      "300 391 Loss: 0.250 | Acc: 92.400% (35600/38528)\n",
      "0 100 Loss: 1.154 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 127\n",
      "0 391 Loss: 0.212 | Acc: 93.750% (120/128)\n",
      "100 391 Loss: 0.236 | Acc: 92.814% (11999/12928)\n",
      "200 391 Loss: 0.235 | Acc: 92.868% (23893/25728)\n",
      "300 391 Loss: 0.240 | Acc: 92.759% (35738/38528)\n",
      "0 100 Loss: 1.270 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 128\n",
      "0 391 Loss: 0.120 | Acc: 97.656% (125/128)\n",
      "100 391 Loss: 0.212 | Acc: 93.820% (12129/12928)\n",
      "200 391 Loss: 0.210 | Acc: 93.797% (24132/25728)\n",
      "300 391 Loss: 0.218 | Acc: 93.553% (36044/38528)\n",
      "0 100 Loss: 1.262 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 129\n",
      "0 391 Loss: 0.214 | Acc: 93.750% (120/128)\n",
      "100 391 Loss: 0.215 | Acc: 93.417% (12077/12928)\n",
      "200 391 Loss: 0.208 | Acc: 93.870% (24151/25728)\n",
      "300 391 Loss: 0.211 | Acc: 93.742% (36117/38528)\n",
      "0 100 Loss: 1.169 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 130\n",
      "0 391 Loss: 0.177 | Acc: 96.094% (123/128)\n",
      "100 391 Loss: 0.174 | Acc: 94.864% (12264/12928)\n",
      "200 391 Loss: 0.186 | Acc: 94.555% (24327/25728)\n",
      "300 391 Loss: 0.195 | Acc: 94.235% (36307/38528)\n",
      "0 100 Loss: 1.169 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 131\n",
      "0 391 Loss: 0.261 | Acc: 94.531% (121/128)\n",
      "100 391 Loss: 0.172 | Acc: 95.019% (12284/12928)\n",
      "200 391 Loss: 0.179 | Acc: 94.722% (24370/25728)\n",
      "300 391 Loss: 0.189 | Acc: 94.422% (36379/38528)\n",
      "0 100 Loss: 1.137 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 132\n",
      "0 391 Loss: 0.225 | Acc: 94.531% (121/128)\n",
      "100 391 Loss: 0.189 | Acc: 94.454% (12211/12928)\n",
      "200 391 Loss: 0.186 | Acc: 94.539% (24323/25728)\n",
      "300 391 Loss: 0.190 | Acc: 94.391% (36367/38528)\n",
      "0 100 Loss: 1.254 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 133\n",
      "0 391 Loss: 0.162 | Acc: 95.312% (122/128)\n",
      "100 391 Loss: 0.174 | Acc: 94.856% (12263/12928)\n",
      "200 391 Loss: 0.176 | Acc: 94.893% (24414/25728)\n",
      "300 391 Loss: 0.178 | Acc: 94.853% (36545/38528)\n",
      "0 100 Loss: 1.236 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 134\n",
      "0 391 Loss: 0.110 | Acc: 96.875% (124/128)\n",
      "100 391 Loss: 0.168 | Acc: 95.111% (12296/12928)\n",
      "200 391 Loss: 0.161 | Acc: 95.495% (24569/25728)\n",
      "300 391 Loss: 0.163 | Acc: 95.432% (36768/38528)\n",
      "0 100 Loss: 1.158 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 135\n",
      "0 391 Loss: 0.195 | Acc: 96.094% (123/128)\n",
      "100 391 Loss: 0.143 | Acc: 95.854% (12392/12928)\n",
      "200 391 Loss: 0.151 | Acc: 95.655% (24610/25728)\n",
      "300 391 Loss: 0.156 | Acc: 95.476% (36785/38528)\n",
      "0 100 Loss: 1.544 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 136\n",
      "0 391 Loss: 0.192 | Acc: 95.312% (122/128)\n",
      "100 391 Loss: 0.162 | Acc: 95.537% (12351/12928)\n",
      "200 391 Loss: 0.152 | Acc: 95.721% (24627/25728)\n",
      "300 391 Loss: 0.159 | Acc: 95.567% (36820/38528)\n",
      "0 100 Loss: 1.225 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 137\n",
      "0 391 Loss: 0.155 | Acc: 96.875% (124/128)\n",
      "100 391 Loss: 0.139 | Acc: 96.202% (12437/12928)\n",
      "200 391 Loss: 0.146 | Acc: 95.950% (24686/25728)\n",
      "300 391 Loss: 0.145 | Acc: 95.920% (36956/38528)\n",
      "0 100 Loss: 1.325 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 138\n",
      "0 391 Loss: 0.133 | Acc: 96.094% (123/128)\n",
      "100 391 Loss: 0.124 | Acc: 96.728% (12505/12928)\n",
      "200 391 Loss: 0.125 | Acc: 96.626% (24860/25728)\n",
      "300 391 Loss: 0.131 | Acc: 96.400% (37141/38528)\n",
      "0 100 Loss: 1.418 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 139\n",
      "0 391 Loss: 0.065 | Acc: 99.219% (127/128)\n",
      "100 391 Loss: 0.126 | Acc: 96.542% (12481/12928)\n",
      "200 391 Loss: 0.124 | Acc: 96.642% (24864/25728)\n",
      "300 391 Loss: 0.128 | Acc: 96.488% (37175/38528)\n",
      "0 100 Loss: 1.818 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 140\n",
      "0 391 Loss: 0.148 | Acc: 94.531% (121/128)\n",
      "100 391 Loss: 0.139 | Acc: 96.210% (12438/12928)\n",
      "200 391 Loss: 0.131 | Acc: 96.440% (24812/25728)\n",
      "300 391 Loss: 0.133 | Acc: 96.379% (37133/38528)\n",
      "0 100 Loss: 1.335 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 141\n",
      "0 391 Loss: 0.101 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.114 | Acc: 97.115% (12555/12928)\n",
      "200 391 Loss: 0.114 | Acc: 96.984% (24952/25728)\n",
      "300 391 Loss: 0.116 | Acc: 96.953% (37354/38528)\n",
      "0 100 Loss: 1.302 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 142\n",
      "0 391 Loss: 0.153 | Acc: 93.750% (120/128)\n",
      "100 391 Loss: 0.099 | Acc: 97.556% (12612/12928)\n",
      "200 391 Loss: 0.098 | Acc: 97.544% (25096/25728)\n",
      "300 391 Loss: 0.100 | Acc: 97.464% (37551/38528)\n",
      "0 100 Loss: 1.215 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 143\n",
      "0 391 Loss: 0.090 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.075 | Acc: 98.383% (12719/12928)\n",
      "200 391 Loss: 0.078 | Acc: 98.228% (25272/25728)\n",
      "300 391 Loss: 0.081 | Acc: 98.100% (37796/38528)\n",
      "0 100 Loss: 1.113 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 144\n",
      "0 391 Loss: 0.143 | Acc: 96.094% (123/128)\n",
      "100 391 Loss: 0.088 | Acc: 97.687% (12629/12928)\n",
      "200 391 Loss: 0.086 | Acc: 97.812% (25165/25728)\n",
      "300 391 Loss: 0.088 | Acc: 97.763% (37666/38528)\n",
      "0 100 Loss: 1.136 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 145\n",
      "0 391 Loss: 0.072 | Acc: 99.219% (127/128)\n",
      "100 391 Loss: 0.076 | Acc: 98.244% (12701/12928)\n",
      "200 391 Loss: 0.073 | Acc: 98.286% (25287/25728)\n",
      "300 391 Loss: 0.075 | Acc: 98.277% (37864/38528)\n",
      "0 100 Loss: 1.072 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 146\n",
      "0 391 Loss: 0.053 | Acc: 97.656% (125/128)\n",
      "100 391 Loss: 0.061 | Acc: 98.755% (12767/12928)\n",
      "200 391 Loss: 0.065 | Acc: 98.562% (25358/25728)\n",
      "300 391 Loss: 0.070 | Acc: 98.360% (37896/38528)\n",
      "0 100 Loss: 1.175 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 147\n",
      "0 391 Loss: 0.094 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.066 | Acc: 98.608% (12748/12928)\n",
      "200 391 Loss: 0.067 | Acc: 98.515% (25346/25728)\n",
      "300 391 Loss: 0.066 | Acc: 98.559% (37973/38528)\n",
      "0 100 Loss: 1.037 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 148\n",
      "0 391 Loss: 0.120 | Acc: 95.312% (122/128)\n",
      "100 391 Loss: 0.058 | Acc: 98.731% (12764/12928)\n",
      "200 391 Loss: 0.056 | Acc: 98.815% (25423/25728)\n",
      "300 391 Loss: 0.056 | Acc: 98.824% (38075/38528)\n",
      "0 100 Loss: 0.989 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 149\n",
      "0 391 Loss: 0.033 | Acc: 99.219% (127/128)\n",
      "100 391 Loss: 0.057 | Acc: 98.755% (12767/12928)\n",
      "200 391 Loss: 0.054 | Acc: 98.877% (25439/25728)\n",
      "300 391 Loss: 0.054 | Acc: 98.889% (38100/38528)\n",
      "0 100 Loss: 1.113 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 150\n",
      "0 391 Loss: 0.023 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.041 | Acc: 99.335% (12842/12928)\n",
      "200 391 Loss: 0.039 | Acc: 99.374% (25567/25728)\n",
      "300 391 Loss: 0.039 | Acc: 99.346% (38276/38528)\n",
      "0 100 Loss: 1.050 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 151\n",
      "0 391 Loss: 0.024 | Acc: 100.000% (128/128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 391 Loss: 0.037 | Acc: 99.389% (12849/12928)\n",
      "200 391 Loss: 0.034 | Acc: 99.483% (25595/25728)\n",
      "300 391 Loss: 0.035 | Acc: 99.447% (38315/38528)\n",
      "0 100 Loss: 0.948 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 152\n",
      "0 391 Loss: 0.026 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.033 | Acc: 99.428% (12854/12928)\n",
      "200 391 Loss: 0.032 | Acc: 99.510% (25602/25728)\n",
      "300 391 Loss: 0.030 | Acc: 99.574% (38364/38528)\n",
      "0 100 Loss: 1.090 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 153\n",
      "0 391 Loss: 0.019 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.027 | Acc: 99.606% (12877/12928)\n",
      "200 391 Loss: 0.024 | Acc: 99.705% (25652/25728)\n",
      "300 391 Loss: 0.024 | Acc: 99.712% (38417/38528)\n",
      "0 100 Loss: 1.080 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 154\n",
      "0 391 Loss: 0.041 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.023 | Acc: 99.760% (12897/12928)\n",
      "200 391 Loss: 0.022 | Acc: 99.763% (25667/25728)\n",
      "300 391 Loss: 0.023 | Acc: 99.746% (38430/38528)\n",
      "0 100 Loss: 1.018 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 155\n",
      "0 391 Loss: 0.024 | Acc: 99.219% (127/128)\n",
      "100 391 Loss: 0.021 | Acc: 99.783% (12900/12928)\n",
      "200 391 Loss: 0.020 | Acc: 99.775% (25670/25728)\n",
      "300 391 Loss: 0.021 | Acc: 99.772% (38440/38528)\n",
      "0 100 Loss: 0.933 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 156\n",
      "0 391 Loss: 0.024 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.018 | Acc: 99.869% (12911/12928)\n",
      "200 391 Loss: 0.018 | Acc: 99.872% (25695/25728)\n",
      "300 391 Loss: 0.018 | Acc: 99.870% (38478/38528)\n",
      "0 100 Loss: 1.077 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 157\n",
      "0 391 Loss: 0.018 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.014 | Acc: 99.954% (12922/12928)\n",
      "200 391 Loss: 0.014 | Acc: 99.926% (25709/25728)\n",
      "300 391 Loss: 0.016 | Acc: 99.870% (38478/38528)\n",
      "0 100 Loss: 0.898 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 158\n",
      "0 391 Loss: 0.019 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.014 | Acc: 99.915% (12917/12928)\n",
      "200 391 Loss: 0.014 | Acc: 99.918% (25707/25728)\n",
      "300 391 Loss: 0.014 | Acc: 99.914% (38495/38528)\n",
      "0 100 Loss: 0.930 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 159\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.011 | Acc: 99.946% (12921/12928)\n",
      "200 391 Loss: 0.012 | Acc: 99.934% (25711/25728)\n",
      "300 391 Loss: 0.013 | Acc: 99.917% (38496/38528)\n",
      "0 100 Loss: 0.900 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 160\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.012 | Acc: 99.946% (12921/12928)\n",
      "200 391 Loss: 0.012 | Acc: 99.953% (25716/25728)\n",
      "300 391 Loss: 0.013 | Acc: 99.943% (38506/38528)\n",
      "0 100 Loss: 0.899 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 161\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.012 | Acc: 99.930% (12919/12928)\n",
      "200 391 Loss: 0.012 | Acc: 99.930% (25710/25728)\n",
      "300 391 Loss: 0.012 | Acc: 99.943% (38506/38528)\n",
      "0 100 Loss: 0.925 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 162\n",
      "0 391 Loss: 0.012 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.012 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.957% (25717/25728)\n",
      "300 391 Loss: 0.012 | Acc: 99.940% (38505/38528)\n",
      "0 100 Loss: 0.885 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 163\n",
      "0 391 Loss: 0.010 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.011 | Acc: 99.930% (12919/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.934% (25711/25728)\n",
      "300 391 Loss: 0.012 | Acc: 99.930% (38501/38528)\n",
      "0 100 Loss: 0.962 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 164\n",
      "0 391 Loss: 0.019 | Acc: 99.219% (127/128)\n",
      "100 391 Loss: 0.011 | Acc: 99.946% (12921/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.957% (25717/25728)\n",
      "300 391 Loss: 0.011 | Acc: 99.943% (38506/38528)\n",
      "0 100 Loss: 0.864 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 165\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.011 | Acc: 99.961% (12923/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.957% (25717/25728)\n",
      "300 391 Loss: 0.011 | Acc: 99.958% (38512/38528)\n",
      "0 100 Loss: 0.834 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 166\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.953% (25716/25728)\n",
      "300 391 Loss: 0.011 | Acc: 99.951% (38509/38528)\n",
      "0 100 Loss: 0.871 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 167\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.992% (12927/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.977% (25722/25728)\n",
      "300 391 Loss: 0.011 | Acc: 99.964% (38514/38528)\n",
      "0 100 Loss: 0.870 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 168\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.985% (12926/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.966% (38515/38528)\n",
      "0 100 Loss: 0.855 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 169\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.938% (12920/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.942% (25713/25728)\n",
      "300 391 Loss: 0.011 | Acc: 99.945% (38507/38528)\n",
      "0 100 Loss: 0.859 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 170\n",
      "0 391 Loss: 0.005 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.992% (12927/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.981% (25723/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.958% (38512/38528)\n",
      "0 100 Loss: 0.880 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 171\n",
      "0 391 Loss: 0.006 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.954% (12922/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.969% (25720/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.966% (38515/38528)\n",
      "0 100 Loss: 0.832 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 172\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.965% (25719/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.969% (38516/38528)\n",
      "0 100 Loss: 0.854 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 173\n",
      "0 391 Loss: 0.012 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.992% (12927/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.964% (38514/38528)\n",
      "0 100 Loss: 0.851 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 174\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.969% (25720/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.953% (38510/38528)\n",
      "0 100 Loss: 0.840 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 175\n",
      "0 391 Loss: 0.016 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.961% (25718/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.958% (38512/38528)\n",
      "0 100 Loss: 0.848 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 176\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.992% (12927/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.981% (25723/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.974% (38518/38528)\n",
      "0 100 Loss: 0.853 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 177\n",
      "0 391 Loss: 0.005 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.961% (25718/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.969% (38516/38528)\n",
      "0 100 Loss: 0.826 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 178\n",
      "0 391 Loss: 0.006 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.961% (25718/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.956% (38511/38528)\n",
      "0 100 Loss: 0.818 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 179\n",
      "0 391 Loss: 0.010 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.985% (12926/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.977% (38519/38528)\n",
      "0 100 Loss: 0.834 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 180\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.954% (12922/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.953% (25716/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.961% (38513/38528)\n",
      "0 100 Loss: 0.835 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 181\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.954% (12922/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.942% (25713/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.953% (38510/38528)\n",
      "0 100 Loss: 0.841 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 182\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.969% (25720/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.964% (38514/38528)\n",
      "0 100 Loss: 0.847 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 183\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.946% (12921/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.961% (25718/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.961% (38513/38528)\n",
      "0 100 Loss: 0.837 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 184\n",
      "0 391 Loss: 0.011 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.977% (25722/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.971% (38517/38528)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100 Loss: 0.848 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 185\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.961% (12923/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.966% (38515/38528)\n",
      "0 100 Loss: 0.846 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 186\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.969% (25720/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.958% (38512/38528)\n",
      "0 100 Loss: 0.823 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 187\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.992% (12927/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.988% (25725/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.971% (38517/38528)\n",
      "0 100 Loss: 0.849 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 188\n",
      "0 391 Loss: 0.012 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.961% (12923/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.977% (38519/38528)\n",
      "0 100 Loss: 0.842 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 189\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.977% (25722/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.982% (38521/38528)\n",
      "0 100 Loss: 0.845 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 190\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.977% (38519/38528)\n",
      "0 100 Loss: 0.831 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 191\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.981% (25723/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.971% (38517/38528)\n",
      "0 100 Loss: 0.823 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 192\n",
      "0 391 Loss: 0.023 | Acc: 99.219% (127/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.974% (38518/38528)\n",
      "0 100 Loss: 0.834 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 193\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.965% (25719/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.969% (38516/38528)\n",
      "0 100 Loss: 0.830 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 194\n",
      "0 391 Loss: 0.006 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.971% (38517/38528)\n",
      "0 100 Loss: 0.830 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 195\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.985% (12926/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.979% (38520/38528)\n",
      "0 100 Loss: 0.838 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 196\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.985% (12926/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.981% (25723/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.977% (38519/38528)\n",
      "0 100 Loss: 0.846 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 197\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.974% (38518/38528)\n",
      "0 100 Loss: 0.844 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 198\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.977% (25722/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.977% (38519/38528)\n",
      "0 100 Loss: 0.841 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 199\n",
      "0 391 Loss: 0.014 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.961% (12923/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.969% (25720/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.974% (38518/38528)\n",
      "0 100 Loss: 0.841 | Acc: 75.000% (75/100)\n"
     ]
    }
   ],
   "source": [
    "args.block = \"RESNET\"\n",
    "net = ResNet18(block=args.block, num_classes=100 if args.dataset == 'cifar100' else 10)\n",
    "resnet_accuracy = run_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.block = \"SE_SA_1\"\n",
    "net = ResNet18(block=args.block, num_classes=100 if args.dataset == 'cifar100' else 10)\n",
    "se_sa_accuracy = run_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61864f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#args.block = \"SEC_SA_1\"\n",
    "#net = ResNet18(block=args.block, num_classes=100 if args.dataset == 'cifar100' else 10)\n",
    "#sec_sa_accuracy = run_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037551d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.block = \"CBAM_1\"\n",
    "net = ResNet18(block=args.block, num_classes=100 if args.dataset == 'cifar100' else 10)\n",
    "cbam_accuracy = run_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9587043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model :  ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "      (image_module): NewBlock(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=8, bias=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=8, out_features=64, bias=False)\n",
      "          (3): Sigmoid()\n",
      "        )\n",
      "        (sg): SpatialGate(\n",
      "          (compress): ChannelPool()\n",
      "          (spatial): BasicConv(\n",
      "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "      (image_module): NewBlock(\n",
      "        (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "        (fc): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=8, bias=False)\n",
      "          (1): ReLU(inplace=True)\n",
      "          (2): Linear(in_features=8, out_features=64, bias=False)\n",
      "          (3): Sigmoid()\n",
      "        )\n",
      "        (sg): SpatialGate(\n",
      "          (compress): ChannelPool()\n",
      "          (spatial): BasicConv(\n",
      "            (conv): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "            (bn): BatchNorm2d(1, eps=1e-05, momentum=0.01, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
      ")\n",
      "\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 391 Loss: 4.745 | Acc: 0.781% (1/128)\n",
      "100 391 Loss: 4.380 | Acc: 5.012% (648/12928)\n",
      "200 391 Loss: 4.162 | Acc: 6.744% (1735/25728)\n",
      "300 391 Loss: 4.030 | Acc: 8.142% (3137/38528)\n",
      "0 100 Loss: 3.835 | Acc: 13.000% (13/100)\n",
      "\n",
      "Epoch: 1\n",
      "0 391 Loss: 3.909 | Acc: 11.719% (15/128)\n",
      "100 391 Loss: 3.503 | Acc: 15.292% (1977/12928)\n",
      "200 391 Loss: 3.428 | Acc: 16.791% (4320/25728)\n",
      "300 391 Loss: 3.375 | Acc: 17.782% (6851/38528)\n",
      "0 100 Loss: 3.336 | Acc: 19.000% (19/100)\n",
      "\n",
      "Epoch: 2\n",
      "0 391 Loss: 3.124 | Acc: 21.875% (28/128)\n",
      "100 391 Loss: 2.988 | Acc: 25.124% (3248/12928)\n",
      "200 391 Loss: 2.932 | Acc: 25.995% (6688/25728)\n",
      "300 391 Loss: 2.878 | Acc: 26.947% (10382/38528)\n",
      "0 100 Loss: 2.931 | Acc: 28.000% (28/100)\n",
      "\n",
      "Epoch: 3\n",
      "0 391 Loss: 2.507 | Acc: 35.156% (45/128)\n",
      "100 391 Loss: 2.500 | Acc: 34.568% (4469/12928)\n",
      "200 391 Loss: 2.458 | Acc: 35.549% (9146/25728)\n",
      "300 391 Loss: 2.409 | Acc: 36.529% (14074/38528)\n",
      "0 100 Loss: 2.544 | Acc: 30.000% (30/100)\n",
      "\n",
      "Epoch: 4\n",
      "0 391 Loss: 2.193 | Acc: 40.625% (52/128)\n",
      "100 391 Loss: 2.111 | Acc: 42.752% (5527/12928)\n",
      "200 391 Loss: 2.084 | Acc: 43.505% (11193/25728)\n",
      "300 391 Loss: 2.064 | Acc: 44.111% (16995/38528)\n",
      "0 100 Loss: 2.308 | Acc: 40.000% (40/100)\n",
      "\n",
      "Epoch: 5\n",
      "0 391 Loss: 1.868 | Acc: 42.188% (54/128)\n",
      "100 391 Loss: 1.853 | Acc: 48.724% (6299/12928)\n",
      "200 391 Loss: 1.854 | Acc: 48.729% (12537/25728)\n",
      "300 391 Loss: 1.853 | Acc: 48.689% (18759/38528)\n",
      "0 100 Loss: 2.310 | Acc: 41.000% (41/100)\n",
      "\n",
      "Epoch: 6\n",
      "0 391 Loss: 1.684 | Acc: 54.688% (70/128)\n",
      "100 391 Loss: 1.684 | Acc: 52.978% (6849/12928)\n",
      "200 391 Loss: 1.690 | Acc: 52.581% (13528/25728)\n",
      "300 391 Loss: 1.698 | Acc: 52.300% (20150/38528)\n",
      "0 100 Loss: 1.717 | Acc: 54.000% (54/100)\n",
      "\n",
      "Epoch: 7\n",
      "0 391 Loss: 1.492 | Acc: 55.469% (71/128)\n",
      "100 391 Loss: 1.574 | Acc: 55.384% (7160/12928)\n",
      "200 391 Loss: 1.598 | Acc: 55.018% (14155/25728)\n",
      "300 391 Loss: 1.598 | Acc: 55.074% (21219/38528)\n",
      "0 100 Loss: 1.895 | Acc: 52.000% (52/100)\n",
      "\n",
      "Epoch: 8\n",
      "0 391 Loss: 1.562 | Acc: 58.594% (75/128)\n",
      "100 391 Loss: 1.510 | Acc: 57.116% (7384/12928)\n",
      "200 391 Loss: 1.531 | Acc: 56.347% (14497/25728)\n",
      "300 391 Loss: 1.526 | Acc: 56.530% (21780/38528)\n",
      "0 100 Loss: 2.017 | Acc: 50.000% (50/100)\n",
      "\n",
      "Epoch: 9\n",
      "0 391 Loss: 1.280 | Acc: 65.625% (84/128)\n",
      "100 391 Loss: 1.434 | Acc: 58.965% (7623/12928)\n",
      "200 391 Loss: 1.442 | Acc: 58.563% (15067/25728)\n",
      "300 391 Loss: 1.454 | Acc: 58.212% (22428/38528)\n",
      "0 100 Loss: 1.841 | Acc: 52.000% (52/100)\n",
      "\n",
      "Epoch: 10\n",
      "0 391 Loss: 1.368 | Acc: 57.031% (73/128)\n",
      "100 391 Loss: 1.365 | Acc: 60.953% (7880/12928)\n",
      "200 391 Loss: 1.398 | Acc: 60.494% (15564/25728)\n",
      "300 391 Loss: 1.413 | Acc: 60.117% (23162/38528)\n",
      "0 100 Loss: 1.890 | Acc: 51.000% (51/100)\n",
      "\n",
      "Epoch: 11\n",
      "0 391 Loss: 1.320 | Acc: 59.375% (76/128)\n",
      "100 391 Loss: 1.331 | Acc: 61.904% (8003/12928)\n",
      "200 391 Loss: 1.356 | Acc: 61.365% (15788/25728)\n",
      "300 391 Loss: 1.359 | Acc: 61.293% (23615/38528)\n",
      "0 100 Loss: 1.708 | Acc: 55.000% (55/100)\n",
      "\n",
      "Epoch: 12\n",
      "0 391 Loss: 1.267 | Acc: 66.406% (85/128)\n",
      "100 391 Loss: 1.270 | Acc: 63.366% (8192/12928)\n",
      "200 391 Loss: 1.301 | Acc: 62.543% (16091/25728)\n",
      "300 391 Loss: 1.317 | Acc: 62.142% (23942/38528)\n",
      "0 100 Loss: 1.712 | Acc: 54.000% (54/100)\n",
      "\n",
      "Epoch: 13\n",
      "0 391 Loss: 1.362 | Acc: 62.500% (80/128)\n",
      "100 391 Loss: 1.238 | Acc: 64.194% (8299/12928)\n",
      "200 391 Loss: 1.271 | Acc: 63.254% (16274/25728)\n",
      "300 391 Loss: 1.288 | Acc: 62.793% (24193/38528)\n",
      "0 100 Loss: 1.524 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 14\n",
      "0 391 Loss: 1.455 | Acc: 58.594% (75/128)\n",
      "100 391 Loss: 1.214 | Acc: 64.797% (8377/12928)\n",
      "200 391 Loss: 1.247 | Acc: 63.926% (16447/25728)\n",
      "300 391 Loss: 1.252 | Acc: 63.831% (24593/38528)\n",
      "0 100 Loss: 1.647 | Acc: 55.000% (55/100)\n",
      "\n",
      "Epoch: 15\n",
      "0 391 Loss: 1.052 | Acc: 67.188% (86/128)\n",
      "100 391 Loss: 1.193 | Acc: 64.968% (8399/12928)\n",
      "200 391 Loss: 1.225 | Acc: 64.280% (16538/25728)\n",
      "300 391 Loss: 1.239 | Acc: 63.831% (24593/38528)\n",
      "0 100 Loss: 1.902 | Acc: 49.000% (49/100)\n",
      "\n",
      "Epoch: 16\n",
      "0 391 Loss: 1.124 | Acc: 67.188% (86/128)\n",
      "100 391 Loss: 1.173 | Acc: 65.548% (8474/12928)\n",
      "200 391 Loss: 1.205 | Acc: 64.960% (16713/25728)\n",
      "300 391 Loss: 1.224 | Acc: 64.550% (24870/38528)\n",
      "0 100 Loss: 1.722 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 17\n",
      "0 391 Loss: 1.131 | Acc: 67.188% (86/128)\n",
      "100 391 Loss: 1.126 | Acc: 67.543% (8732/12928)\n",
      "200 391 Loss: 1.158 | Acc: 66.589% (17132/25728)\n",
      "300 391 Loss: 1.184 | Acc: 65.718% (25320/38528)\n",
      "0 100 Loss: 1.628 | Acc: 59.000% (59/100)\n",
      "\n",
      "Epoch: 18\n",
      "0 391 Loss: 1.213 | Acc: 65.625% (84/128)\n",
      "100 391 Loss: 1.152 | Acc: 65.934% (8524/12928)\n",
      "200 391 Loss: 1.166 | Acc: 65.944% (16966/25728)\n",
      "300 391 Loss: 1.182 | Acc: 65.589% (25270/38528)\n",
      "0 100 Loss: 1.621 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 19\n",
      "0 391 Loss: 0.974 | Acc: 64.844% (83/128)\n",
      "100 391 Loss: 1.114 | Acc: 67.427% (8717/12928)\n",
      "200 391 Loss: 1.152 | Acc: 66.484% (17105/25728)\n",
      "300 391 Loss: 1.157 | Acc: 66.518% (25628/38528)\n",
      "0 100 Loss: 1.603 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 20\n",
      "0 391 Loss: 1.199 | Acc: 63.281% (81/128)\n",
      "100 391 Loss: 1.087 | Acc: 68.340% (8835/12928)\n",
      "200 391 Loss: 1.113 | Acc: 67.498% (17366/25728)\n",
      "300 391 Loss: 1.131 | Acc: 67.128% (25863/38528)\n",
      "0 100 Loss: 1.488 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 21\n",
      "0 391 Loss: 1.027 | Acc: 68.750% (88/128)\n",
      "100 391 Loss: 1.058 | Acc: 69.114% (8935/12928)\n",
      "200 391 Loss: 1.100 | Acc: 68.148% (17533/25728)\n",
      "300 391 Loss: 1.122 | Acc: 67.444% (25985/38528)\n",
      "0 100 Loss: 1.662 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 22\n",
      "0 391 Loss: 1.076 | Acc: 71.875% (92/128)\n",
      "100 391 Loss: 1.075 | Acc: 68.340% (8835/12928)\n",
      "200 391 Loss: 1.095 | Acc: 67.844% (17455/25728)\n",
      "300 391 Loss: 1.113 | Acc: 67.421% (25976/38528)\n",
      "0 100 Loss: 1.675 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 23\n",
      "0 391 Loss: 0.901 | Acc: 75.000% (96/128)\n",
      "100 391 Loss: 1.048 | Acc: 68.951% (8914/12928)\n",
      "200 391 Loss: 1.090 | Acc: 68.066% (17512/25728)\n",
      "300 391 Loss: 1.094 | Acc: 67.878% (26152/38528)\n",
      "0 100 Loss: 1.643 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 24\n",
      "0 391 Loss: 0.972 | Acc: 68.750% (88/128)\n",
      "100 391 Loss: 1.012 | Acc: 70.320% (9091/12928)\n",
      "200 391 Loss: 1.052 | Acc: 69.333% (17838/25728)\n",
      "300 391 Loss: 1.073 | Acc: 68.799% (26507/38528)\n",
      "0 100 Loss: 1.397 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 25\n",
      "0 391 Loss: 0.976 | Acc: 71.094% (91/128)\n",
      "100 391 Loss: 0.999 | Acc: 70.459% (9109/12928)\n",
      "200 391 Loss: 1.046 | Acc: 69.251% (17817/25728)\n",
      "300 391 Loss: 1.071 | Acc: 68.649% (26449/38528)\n",
      "0 100 Loss: 1.669 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 26\n",
      "0 391 Loss: 1.012 | Acc: 73.438% (94/128)\n",
      "100 391 Loss: 0.987 | Acc: 71.055% (9186/12928)\n",
      "200 391 Loss: 1.042 | Acc: 69.671% (17925/25728)\n",
      "300 391 Loss: 1.057 | Acc: 69.168% (26649/38528)\n",
      "0 100 Loss: 1.616 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 27\n",
      "0 391 Loss: 0.906 | Acc: 71.875% (92/128)\n",
      "100 391 Loss: 0.986 | Acc: 70.869% (9162/12928)\n",
      "200 391 Loss: 1.022 | Acc: 69.714% (17936/25728)\n",
      "300 391 Loss: 1.043 | Acc: 69.054% (26605/38528)\n",
      "0 100 Loss: 1.764 | Acc: 57.000% (57/100)\n",
      "\n",
      "Epoch: 28\n",
      "0 391 Loss: 1.219 | Acc: 65.625% (84/128)\n",
      "100 391 Loss: 0.995 | Acc: 70.862% (9161/12928)\n",
      "200 391 Loss: 1.026 | Acc: 69.967% (18001/25728)\n",
      "300 391 Loss: 1.052 | Acc: 69.272% (26689/38528)\n",
      "0 100 Loss: 1.550 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 29\n",
      "0 391 Loss: 0.941 | Acc: 71.094% (91/128)\n",
      "100 391 Loss: 0.974 | Acc: 70.993% (9178/12928)\n",
      "200 391 Loss: 1.011 | Acc: 70.048% (18022/25728)\n",
      "300 391 Loss: 1.038 | Acc: 69.324% (26709/38528)\n",
      "0 100 Loss: 1.675 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 30\n",
      "0 391 Loss: 0.727 | Acc: 83.594% (107/128)\n",
      "100 391 Loss: 0.933 | Acc: 72.811% (9413/12928)\n",
      "200 391 Loss: 0.987 | Acc: 70.977% (18261/25728)\n",
      "300 391 Loss: 1.021 | Acc: 70.066% (26995/38528)\n",
      "0 100 Loss: 1.506 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 31\n",
      "0 391 Loss: 1.032 | Acc: 65.625% (84/128)\n",
      "100 391 Loss: 0.961 | Acc: 71.620% (9259/12928)\n",
      "200 391 Loss: 0.985 | Acc: 71.098% (18292/25728)\n",
      "300 391 Loss: 1.015 | Acc: 70.224% (27056/38528)\n",
      "0 100 Loss: 1.539 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 32\n",
      "0 391 Loss: 0.891 | Acc: 72.656% (93/128)\n",
      "100 391 Loss: 0.937 | Acc: 72.718% (9401/12928)\n",
      "200 391 Loss: 0.982 | Acc: 71.432% (18378/25728)\n",
      "300 391 Loss: 1.003 | Acc: 70.601% (27201/38528)\n",
      "0 100 Loss: 1.787 | Acc: 57.000% (57/100)\n",
      "\n",
      "Epoch: 33\n",
      "0 391 Loss: 0.849 | Acc: 72.656% (93/128)\n",
      "100 391 Loss: 0.930 | Acc: 71.697% (9269/12928)\n",
      "200 391 Loss: 0.964 | Acc: 71.199% (18318/25728)\n",
      "300 391 Loss: 0.997 | Acc: 70.499% (27162/38528)\n",
      "0 100 Loss: 1.666 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 34\n",
      "0 391 Loss: 0.808 | Acc: 75.781% (97/128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 391 Loss: 0.939 | Acc: 72.099% (9321/12928)\n",
      "200 391 Loss: 0.972 | Acc: 71.335% (18353/25728)\n",
      "300 391 Loss: 0.988 | Acc: 70.954% (27337/38528)\n",
      "0 100 Loss: 1.660 | Acc: 60.000% (60/100)\n",
      "\n",
      "Epoch: 35\n",
      "0 391 Loss: 1.012 | Acc: 70.312% (90/128)\n",
      "100 391 Loss: 0.917 | Acc: 73.321% (9479/12928)\n",
      "200 391 Loss: 0.949 | Acc: 72.252% (18589/25728)\n",
      "300 391 Loss: 0.974 | Acc: 71.434% (27522/38528)\n",
      "0 100 Loss: 1.416 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 36\n",
      "0 391 Loss: 1.084 | Acc: 64.062% (82/128)\n",
      "100 391 Loss: 0.883 | Acc: 73.476% (9499/12928)\n",
      "200 391 Loss: 0.931 | Acc: 72.458% (18642/25728)\n",
      "300 391 Loss: 0.962 | Acc: 71.532% (27560/38528)\n",
      "0 100 Loss: 1.831 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 37\n",
      "0 391 Loss: 0.959 | Acc: 71.094% (91/128)\n",
      "100 391 Loss: 0.909 | Acc: 73.438% (9494/12928)\n",
      "200 391 Loss: 0.940 | Acc: 72.221% (18581/25728)\n",
      "300 391 Loss: 0.963 | Acc: 71.626% (27596/38528)\n",
      "0 100 Loss: 1.415 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 38\n",
      "0 391 Loss: 0.844 | Acc: 74.219% (95/128)\n",
      "100 391 Loss: 0.889 | Acc: 73.554% (9509/12928)\n",
      "200 391 Loss: 0.941 | Acc: 71.992% (18522/25728)\n",
      "300 391 Loss: 0.966 | Acc: 71.291% (27467/38528)\n",
      "0 100 Loss: 1.644 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 39\n",
      "0 391 Loss: 1.229 | Acc: 64.844% (83/128)\n",
      "100 391 Loss: 0.920 | Acc: 73.113% (9452/12928)\n",
      "200 391 Loss: 0.937 | Acc: 72.400% (18627/25728)\n",
      "300 391 Loss: 0.953 | Acc: 71.963% (27726/38528)\n",
      "0 100 Loss: 1.879 | Acc: 52.000% (52/100)\n",
      "\n",
      "Epoch: 40\n",
      "0 391 Loss: 1.089 | Acc: 70.312% (90/128)\n",
      "100 391 Loss: 0.874 | Acc: 73.855% (9548/12928)\n",
      "200 391 Loss: 0.914 | Acc: 72.847% (18742/25728)\n",
      "300 391 Loss: 0.942 | Acc: 72.070% (27767/38528)\n",
      "0 100 Loss: 1.879 | Acc: 53.000% (53/100)\n",
      "\n",
      "Epoch: 41\n",
      "0 391 Loss: 0.716 | Acc: 78.125% (100/128)\n",
      "100 391 Loss: 0.899 | Acc: 73.399% (9489/12928)\n",
      "200 391 Loss: 0.933 | Acc: 72.353% (18615/25728)\n",
      "300 391 Loss: 0.941 | Acc: 72.036% (27754/38528)\n",
      "0 100 Loss: 1.561 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 42\n",
      "0 391 Loss: 0.880 | Acc: 75.000% (96/128)\n",
      "100 391 Loss: 0.865 | Acc: 74.343% (9611/12928)\n",
      "200 391 Loss: 0.905 | Acc: 73.212% (18836/25728)\n",
      "300 391 Loss: 0.931 | Acc: 72.407% (27897/38528)\n",
      "0 100 Loss: 1.430 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 43\n",
      "0 391 Loss: 0.939 | Acc: 75.781% (97/128)\n",
      "100 391 Loss: 0.858 | Acc: 74.435% (9623/12928)\n",
      "200 391 Loss: 0.897 | Acc: 73.445% (18896/25728)\n",
      "300 391 Loss: 0.928 | Acc: 72.641% (27987/38528)\n",
      "0 100 Loss: 1.427 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 44\n",
      "0 391 Loss: 0.939 | Acc: 78.125% (100/128)\n",
      "100 391 Loss: 0.839 | Acc: 74.683% (9655/12928)\n",
      "200 391 Loss: 0.886 | Acc: 73.344% (18870/25728)\n",
      "300 391 Loss: 0.909 | Acc: 72.703% (28011/38528)\n",
      "0 100 Loss: 1.871 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 45\n",
      "0 391 Loss: 0.774 | Acc: 79.688% (102/128)\n",
      "100 391 Loss: 0.844 | Acc: 74.822% (9673/12928)\n",
      "200 391 Loss: 0.874 | Acc: 73.958% (19028/25728)\n",
      "300 391 Loss: 0.909 | Acc: 73.121% (28172/38528)\n",
      "0 100 Loss: 1.609 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 46\n",
      "0 391 Loss: 0.671 | Acc: 78.125% (100/128)\n",
      "100 391 Loss: 0.829 | Acc: 74.845% (9676/12928)\n",
      "200 391 Loss: 0.865 | Acc: 73.997% (19038/25728)\n",
      "300 391 Loss: 0.898 | Acc: 73.282% (28234/38528)\n",
      "0 100 Loss: 1.528 | Acc: 60.000% (60/100)\n",
      "\n",
      "Epoch: 47\n",
      "0 391 Loss: 0.546 | Acc: 89.062% (114/128)\n",
      "100 391 Loss: 0.839 | Acc: 75.147% (9715/12928)\n",
      "200 391 Loss: 0.868 | Acc: 74.320% (19121/25728)\n",
      "300 391 Loss: 0.895 | Acc: 73.448% (28298/38528)\n",
      "0 100 Loss: 1.812 | Acc: 53.000% (53/100)\n",
      "\n",
      "Epoch: 48\n",
      "0 391 Loss: 0.846 | Acc: 74.219% (95/128)\n",
      "100 391 Loss: 0.841 | Acc: 74.923% (9686/12928)\n",
      "200 391 Loss: 0.872 | Acc: 73.982% (19034/25728)\n",
      "300 391 Loss: 0.903 | Acc: 73.188% (28198/38528)\n",
      "0 100 Loss: 1.665 | Acc: 54.000% (54/100)\n",
      "\n",
      "Epoch: 49\n",
      "0 391 Loss: 0.914 | Acc: 69.531% (89/128)\n",
      "100 391 Loss: 0.839 | Acc: 74.590% (9643/12928)\n",
      "200 391 Loss: 0.861 | Acc: 73.873% (19006/25728)\n",
      "300 391 Loss: 0.884 | Acc: 73.292% (28238/38528)\n",
      "0 100 Loss: 1.443 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 50\n",
      "0 391 Loss: 0.912 | Acc: 71.094% (91/128)\n",
      "100 391 Loss: 0.796 | Acc: 76.006% (9826/12928)\n",
      "200 391 Loss: 0.834 | Acc: 74.930% (19278/25728)\n",
      "300 391 Loss: 0.864 | Acc: 74.271% (28615/38528)\n",
      "0 100 Loss: 1.311 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 51\n",
      "0 391 Loss: 0.790 | Acc: 78.125% (100/128)\n",
      "100 391 Loss: 0.806 | Acc: 75.634% (9778/12928)\n",
      "200 391 Loss: 0.848 | Acc: 74.712% (19222/25728)\n",
      "300 391 Loss: 0.871 | Acc: 73.923% (28481/38528)\n",
      "0 100 Loss: 1.398 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 52\n",
      "0 391 Loss: 0.920 | Acc: 70.312% (90/128)\n",
      "100 391 Loss: 0.805 | Acc: 75.936% (9817/12928)\n",
      "200 391 Loss: 0.840 | Acc: 75.101% (19322/25728)\n",
      "300 391 Loss: 0.869 | Acc: 74.208% (28591/38528)\n",
      "0 100 Loss: 1.600 | Acc: 57.000% (57/100)\n",
      "\n",
      "Epoch: 53\n",
      "0 391 Loss: 0.641 | Acc: 79.688% (102/128)\n",
      "100 391 Loss: 0.794 | Acc: 76.106% (9839/12928)\n",
      "200 391 Loss: 0.829 | Acc: 75.272% (19366/25728)\n",
      "300 391 Loss: 0.852 | Acc: 74.735% (28794/38528)\n",
      "0 100 Loss: 1.390 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 54\n",
      "0 391 Loss: 0.712 | Acc: 80.469% (103/128)\n",
      "100 391 Loss: 0.791 | Acc: 75.998% (9825/12928)\n",
      "200 391 Loss: 0.826 | Acc: 75.187% (19344/25728)\n",
      "300 391 Loss: 0.852 | Acc: 74.413% (28670/38528)\n",
      "0 100 Loss: 1.502 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 55\n",
      "0 391 Loss: 0.888 | Acc: 75.781% (97/128)\n",
      "100 391 Loss: 0.804 | Acc: 75.920% (9815/12928)\n",
      "200 391 Loss: 0.830 | Acc: 75.342% (19384/25728)\n",
      "300 391 Loss: 0.862 | Acc: 74.489% (28699/38528)\n",
      "0 100 Loss: 1.721 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 56\n",
      "0 391 Loss: 0.511 | Acc: 85.938% (110/128)\n",
      "100 391 Loss: 0.770 | Acc: 76.965% (9950/12928)\n",
      "200 391 Loss: 0.811 | Acc: 76.069% (19571/25728)\n",
      "300 391 Loss: 0.834 | Acc: 75.449% (29069/38528)\n",
      "0 100 Loss: 1.424 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 57\n",
      "0 391 Loss: 0.921 | Acc: 73.438% (94/128)\n",
      "100 391 Loss: 0.781 | Acc: 76.903% (9942/12928)\n",
      "200 391 Loss: 0.802 | Acc: 76.174% (19598/25728)\n",
      "300 391 Loss: 0.825 | Acc: 75.519% (29096/38528)\n",
      "0 100 Loss: 1.174 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 58\n",
      "0 391 Loss: 0.852 | Acc: 71.094% (91/128)\n",
      "100 391 Loss: 0.754 | Acc: 77.375% (10003/12928)\n",
      "200 391 Loss: 0.787 | Acc: 76.391% (19654/25728)\n",
      "300 391 Loss: 0.823 | Acc: 75.345% (29029/38528)\n",
      "0 100 Loss: 1.637 | Acc: 57.000% (57/100)\n",
      "\n",
      "Epoch: 59\n",
      "0 391 Loss: 0.737 | Acc: 77.344% (99/128)\n",
      "100 391 Loss: 0.736 | Acc: 77.700% (10045/12928)\n",
      "200 391 Loss: 0.786 | Acc: 76.112% (19582/25728)\n",
      "300 391 Loss: 0.816 | Acc: 75.397% (29049/38528)\n",
      "0 100 Loss: 1.544 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 60\n",
      "0 391 Loss: 0.707 | Acc: 77.344% (99/128)\n",
      "100 391 Loss: 0.745 | Acc: 77.645% (10038/12928)\n",
      "200 391 Loss: 0.794 | Acc: 76.364% (19647/25728)\n",
      "300 391 Loss: 0.811 | Acc: 75.807% (29207/38528)\n",
      "0 100 Loss: 1.429 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 61\n",
      "0 391 Loss: 0.703 | Acc: 79.688% (102/128)\n",
      "100 391 Loss: 0.737 | Acc: 77.731% (10049/12928)\n",
      "200 391 Loss: 0.773 | Acc: 76.811% (19762/25728)\n",
      "300 391 Loss: 0.805 | Acc: 75.968% (29269/38528)\n",
      "0 100 Loss: 1.689 | Acc: 60.000% (60/100)\n",
      "\n",
      "Epoch: 62\n",
      "0 391 Loss: 0.627 | Acc: 78.906% (101/128)\n",
      "100 391 Loss: 0.729 | Acc: 78.287% (10121/12928)\n",
      "200 391 Loss: 0.768 | Acc: 77.212% (19865/25728)\n",
      "300 391 Loss: 0.799 | Acc: 76.235% (29372/38528)\n",
      "0 100 Loss: 1.611 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 63\n",
      "0 391 Loss: 0.684 | Acc: 80.469% (103/128)\n",
      "100 391 Loss: 0.711 | Acc: 78.496% (10148/12928)\n",
      "200 391 Loss: 0.764 | Acc: 76.866% (19776/25728)\n",
      "300 391 Loss: 0.784 | Acc: 76.311% (29401/38528)\n",
      "0 100 Loss: 1.433 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 64\n",
      "0 391 Loss: 0.590 | Acc: 82.812% (106/128)\n",
      "100 391 Loss: 0.718 | Acc: 78.527% (10152/12928)\n",
      "200 391 Loss: 0.749 | Acc: 77.624% (19971/25728)\n",
      "300 391 Loss: 0.784 | Acc: 76.529% (29485/38528)\n",
      "0 100 Loss: 1.552 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 65\n",
      "0 391 Loss: 0.633 | Acc: 78.125% (100/128)\n",
      "100 391 Loss: 0.723 | Acc: 78.489% (10147/12928)\n",
      "200 391 Loss: 0.747 | Acc: 77.577% (19959/25728)\n",
      "300 391 Loss: 0.775 | Acc: 76.651% (29532/38528)\n",
      "0 100 Loss: 1.789 | Acc: 57.000% (57/100)\n",
      "\n",
      "Epoch: 66\n",
      "0 391 Loss: 0.745 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.706 | Acc: 78.659% (10169/12928)\n",
      "200 391 Loss: 0.748 | Acc: 77.488% (19936/25728)\n",
      "300 391 Loss: 0.763 | Acc: 76.978% (29658/38528)\n",
      "0 100 Loss: 1.785 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 67\n",
      "0 391 Loss: 0.587 | Acc: 84.375% (108/128)\n",
      "100 391 Loss: 0.692 | Acc: 79.084% (10224/12928)\n",
      "200 391 Loss: 0.722 | Acc: 78.389% (20168/25728)\n",
      "300 391 Loss: 0.763 | Acc: 77.126% (29715/38528)\n",
      "0 100 Loss: 1.699 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 68\n",
      "0 391 Loss: 0.860 | Acc: 78.906% (101/128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 391 Loss: 0.706 | Acc: 78.380% (10133/12928)\n",
      "200 391 Loss: 0.730 | Acc: 77.865% (20033/25728)\n",
      "300 391 Loss: 0.750 | Acc: 77.367% (29808/38528)\n",
      "0 100 Loss: 1.417 | Acc: 59.000% (59/100)\n",
      "\n",
      "Epoch: 69\n",
      "0 391 Loss: 0.774 | Acc: 79.688% (102/128)\n",
      "100 391 Loss: 0.675 | Acc: 79.618% (10293/12928)\n",
      "200 391 Loss: 0.723 | Acc: 78.152% (20107/25728)\n",
      "300 391 Loss: 0.752 | Acc: 77.281% (29775/38528)\n",
      "0 100 Loss: 1.581 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 70\n",
      "0 391 Loss: 0.728 | Acc: 74.219% (95/128)\n",
      "100 391 Loss: 0.681 | Acc: 79.626% (10294/12928)\n",
      "200 391 Loss: 0.717 | Acc: 78.560% (20212/25728)\n",
      "300 391 Loss: 0.747 | Acc: 77.621% (29906/38528)\n",
      "0 100 Loss: 1.339 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 71\n",
      "0 391 Loss: 0.592 | Acc: 83.594% (107/128)\n",
      "100 391 Loss: 0.668 | Acc: 79.920% (10332/12928)\n",
      "200 391 Loss: 0.704 | Acc: 78.634% (20231/25728)\n",
      "300 391 Loss: 0.729 | Acc: 77.891% (30010/38528)\n",
      "0 100 Loss: 1.575 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 72\n",
      "0 391 Loss: 0.623 | Acc: 85.156% (109/128)\n",
      "100 391 Loss: 0.674 | Acc: 79.633% (10295/12928)\n",
      "200 391 Loss: 0.692 | Acc: 79.174% (20370/25728)\n",
      "300 391 Loss: 0.728 | Acc: 78.107% (30093/38528)\n",
      "0 100 Loss: 1.500 | Acc: 58.000% (58/100)\n",
      "\n",
      "Epoch: 73\n",
      "0 391 Loss: 0.691 | Acc: 83.594% (107/128)\n",
      "100 391 Loss: 0.668 | Acc: 79.920% (10332/12928)\n",
      "200 391 Loss: 0.694 | Acc: 78.984% (20321/25728)\n",
      "300 391 Loss: 0.725 | Acc: 78.076% (30081/38528)\n",
      "0 100 Loss: 1.314 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 74\n",
      "0 391 Loss: 0.633 | Acc: 82.812% (106/128)\n",
      "100 391 Loss: 0.639 | Acc: 80.724% (10436/12928)\n",
      "200 391 Loss: 0.680 | Acc: 79.532% (20462/25728)\n",
      "300 391 Loss: 0.711 | Acc: 78.688% (30317/38528)\n",
      "0 100 Loss: 1.639 | Acc: 59.000% (59/100)\n",
      "\n",
      "Epoch: 75\n",
      "0 391 Loss: 0.667 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.634 | Acc: 80.492% (10406/12928)\n",
      "200 391 Loss: 0.659 | Acc: 79.800% (20531/25728)\n",
      "300 391 Loss: 0.700 | Acc: 78.652% (30303/38528)\n",
      "0 100 Loss: 1.582 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 76\n",
      "0 391 Loss: 0.623 | Acc: 83.594% (107/128)\n",
      "100 391 Loss: 0.635 | Acc: 80.554% (10414/12928)\n",
      "200 391 Loss: 0.657 | Acc: 80.084% (20604/25728)\n",
      "300 391 Loss: 0.693 | Acc: 78.813% (30365/38528)\n",
      "0 100 Loss: 1.497 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 77\n",
      "0 391 Loss: 0.631 | Acc: 84.375% (108/128)\n",
      "100 391 Loss: 0.633 | Acc: 80.863% (10454/12928)\n",
      "200 391 Loss: 0.657 | Acc: 79.932% (20565/25728)\n",
      "300 391 Loss: 0.685 | Acc: 79.000% (30437/38528)\n",
      "0 100 Loss: 1.640 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 78\n",
      "0 391 Loss: 0.562 | Acc: 82.812% (106/128)\n",
      "100 391 Loss: 0.626 | Acc: 80.910% (10460/12928)\n",
      "200 391 Loss: 0.649 | Acc: 80.224% (20640/25728)\n",
      "300 391 Loss: 0.676 | Acc: 79.607% (30671/38528)\n",
      "0 100 Loss: 1.554 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 79\n",
      "0 391 Loss: 0.621 | Acc: 84.375% (108/128)\n",
      "100 391 Loss: 0.619 | Acc: 81.304% (10511/12928)\n",
      "200 391 Loss: 0.648 | Acc: 80.197% (20633/25728)\n",
      "300 391 Loss: 0.675 | Acc: 79.490% (30626/38528)\n",
      "0 100 Loss: 1.424 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 80\n",
      "0 391 Loss: 0.623 | Acc: 80.469% (103/128)\n",
      "100 391 Loss: 0.591 | Acc: 82.225% (10630/12928)\n",
      "200 391 Loss: 0.613 | Acc: 81.262% (20907/25728)\n",
      "300 391 Loss: 0.651 | Acc: 80.238% (30914/38528)\n",
      "0 100 Loss: 1.611 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 81\n",
      "0 391 Loss: 0.702 | Acc: 78.906% (101/128)\n",
      "100 391 Loss: 0.617 | Acc: 81.227% (10501/12928)\n",
      "200 391 Loss: 0.628 | Acc: 80.904% (20815/25728)\n",
      "300 391 Loss: 0.653 | Acc: 80.124% (30870/38528)\n",
      "0 100 Loss: 1.796 | Acc: 60.000% (60/100)\n",
      "\n",
      "Epoch: 82\n",
      "0 391 Loss: 0.538 | Acc: 82.812% (106/128)\n",
      "100 391 Loss: 0.595 | Acc: 81.830% (10579/12928)\n",
      "200 391 Loss: 0.616 | Acc: 81.126% (20872/25728)\n",
      "300 391 Loss: 0.646 | Acc: 80.339% (30953/38528)\n",
      "0 100 Loss: 1.405 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 83\n",
      "0 391 Loss: 0.627 | Acc: 75.781% (97/128)\n",
      "100 391 Loss: 0.593 | Acc: 81.923% (10591/12928)\n",
      "200 391 Loss: 0.610 | Acc: 81.402% (20943/25728)\n",
      "300 391 Loss: 0.642 | Acc: 80.386% (30971/38528)\n",
      "0 100 Loss: 1.386 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 84\n",
      "0 391 Loss: 0.655 | Acc: 79.688% (102/128)\n",
      "100 391 Loss: 0.588 | Acc: 82.070% (10610/12928)\n",
      "200 391 Loss: 0.597 | Acc: 81.744% (21031/25728)\n",
      "300 391 Loss: 0.618 | Acc: 81.079% (31238/38528)\n",
      "0 100 Loss: 1.482 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 85\n",
      "0 391 Loss: 0.608 | Acc: 80.469% (103/128)\n",
      "100 391 Loss: 0.583 | Acc: 82.503% (10666/12928)\n",
      "200 391 Loss: 0.607 | Acc: 81.643% (21005/25728)\n",
      "300 391 Loss: 0.625 | Acc: 81.097% (31245/38528)\n",
      "0 100 Loss: 1.355 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 86\n",
      "0 391 Loss: 0.819 | Acc: 72.656% (93/128)\n",
      "100 391 Loss: 0.574 | Acc: 82.511% (10667/12928)\n",
      "200 391 Loss: 0.594 | Acc: 81.790% (21043/25728)\n",
      "300 391 Loss: 0.610 | Acc: 81.336% (31337/38528)\n",
      "0 100 Loss: 1.761 | Acc: 59.000% (59/100)\n",
      "\n",
      "Epoch: 87\n",
      "0 391 Loss: 0.596 | Acc: 82.031% (105/128)\n",
      "100 391 Loss: 0.548 | Acc: 83.192% (10755/12928)\n",
      "200 391 Loss: 0.570 | Acc: 82.564% (21242/25728)\n",
      "300 391 Loss: 0.597 | Acc: 81.696% (31476/38528)\n",
      "0 100 Loss: 1.241 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 88\n",
      "0 391 Loss: 0.501 | Acc: 84.375% (108/128)\n",
      "100 391 Loss: 0.517 | Acc: 84.213% (10887/12928)\n",
      "200 391 Loss: 0.545 | Acc: 83.341% (21442/25728)\n",
      "300 391 Loss: 0.578 | Acc: 82.332% (31721/38528)\n",
      "0 100 Loss: 1.134 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 89\n",
      "0 391 Loss: 0.537 | Acc: 82.812% (106/128)\n",
      "100 391 Loss: 0.538 | Acc: 83.130% (10747/12928)\n",
      "200 391 Loss: 0.558 | Acc: 82.525% (21232/25728)\n",
      "300 391 Loss: 0.591 | Acc: 81.567% (31426/38528)\n",
      "0 100 Loss: 1.397 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 90\n",
      "0 391 Loss: 0.569 | Acc: 84.375% (108/128)\n",
      "100 391 Loss: 0.508 | Acc: 84.700% (10950/12928)\n",
      "200 391 Loss: 0.536 | Acc: 83.839% (21570/25728)\n",
      "300 391 Loss: 0.567 | Acc: 82.807% (31904/38528)\n",
      "0 100 Loss: 1.297 | Acc: 70.000% (70/100)\n",
      "\n",
      "Epoch: 91\n",
      "0 391 Loss: 0.603 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.503 | Acc: 84.514% (10926/12928)\n",
      "200 391 Loss: 0.537 | Acc: 83.683% (21530/25728)\n",
      "300 391 Loss: 0.565 | Acc: 82.810% (31905/38528)\n",
      "0 100 Loss: 1.523 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 92\n",
      "0 391 Loss: 0.595 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.501 | Acc: 84.522% (10927/12928)\n",
      "200 391 Loss: 0.523 | Acc: 83.870% (21578/25728)\n",
      "300 391 Loss: 0.544 | Acc: 83.337% (32108/38528)\n",
      "0 100 Loss: 1.509 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 93\n",
      "0 391 Loss: 0.475 | Acc: 85.938% (110/128)\n",
      "100 391 Loss: 0.517 | Acc: 84.042% (10865/12928)\n",
      "200 391 Loss: 0.534 | Acc: 83.671% (21527/25728)\n",
      "300 391 Loss: 0.547 | Acc: 83.251% (32075/38528)\n",
      "0 100 Loss: 1.259 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 94\n",
      "0 391 Loss: 0.561 | Acc: 80.469% (103/128)\n",
      "100 391 Loss: 0.504 | Acc: 84.514% (10926/12928)\n",
      "200 391 Loss: 0.523 | Acc: 83.889% (21583/25728)\n",
      "300 391 Loss: 0.541 | Acc: 83.311% (32098/38528)\n",
      "0 100 Loss: 1.616 | Acc: 59.000% (59/100)\n",
      "\n",
      "Epoch: 95\n",
      "0 391 Loss: 0.532 | Acc: 85.938% (110/128)\n",
      "100 391 Loss: 0.495 | Acc: 84.855% (10970/12928)\n",
      "200 391 Loss: 0.510 | Acc: 84.188% (21660/25728)\n",
      "300 391 Loss: 0.536 | Acc: 83.464% (32157/38528)\n",
      "0 100 Loss: 1.414 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 96\n",
      "0 391 Loss: 0.466 | Acc: 88.281% (113/128)\n",
      "100 391 Loss: 0.470 | Acc: 85.675% (11076/12928)\n",
      "200 391 Loss: 0.497 | Acc: 84.857% (21832/25728)\n",
      "300 391 Loss: 0.527 | Acc: 83.949% (32344/38528)\n",
      "0 100 Loss: 1.416 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 97\n",
      "0 391 Loss: 0.529 | Acc: 85.938% (110/128)\n",
      "100 391 Loss: 0.463 | Acc: 86.030% (11122/12928)\n",
      "200 391 Loss: 0.480 | Acc: 85.463% (21988/25728)\n",
      "300 391 Loss: 0.508 | Acc: 84.513% (32561/38528)\n",
      "0 100 Loss: 1.440 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 98\n",
      "0 391 Loss: 0.534 | Acc: 81.250% (104/128)\n",
      "100 391 Loss: 0.454 | Acc: 85.767% (11088/12928)\n",
      "200 391 Loss: 0.482 | Acc: 85.125% (21901/25728)\n",
      "300 391 Loss: 0.505 | Acc: 84.328% (32490/38528)\n",
      "0 100 Loss: 1.282 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 99\n",
      "0 391 Loss: 0.479 | Acc: 87.500% (112/128)\n",
      "100 391 Loss: 0.450 | Acc: 86.177% (11141/12928)\n",
      "200 391 Loss: 0.458 | Acc: 85.899% (22100/25728)\n",
      "300 391 Loss: 0.484 | Acc: 85.029% (32760/38528)\n",
      "0 100 Loss: 1.467 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 100\n",
      "0 391 Loss: 0.501 | Acc: 82.812% (106/128)\n",
      "100 391 Loss: 0.432 | Acc: 86.881% (11232/12928)\n",
      "200 391 Loss: 0.457 | Acc: 86.004% (22127/25728)\n",
      "300 391 Loss: 0.479 | Acc: 85.338% (32879/38528)\n",
      "0 100 Loss: 1.188 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 101\n",
      "0 391 Loss: 0.527 | Acc: 83.594% (107/128)\n",
      "100 391 Loss: 0.431 | Acc: 86.665% (11204/12928)\n",
      "200 391 Loss: 0.446 | Acc: 86.159% (22167/25728)\n",
      "300 391 Loss: 0.470 | Acc: 85.382% (32896/38528)\n",
      "0 100 Loss: 1.483 | Acc: 66.000% (66/100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 102\n",
      "0 391 Loss: 0.455 | Acc: 88.281% (113/128)\n",
      "100 391 Loss: 0.415 | Acc: 87.392% (11298/12928)\n",
      "200 391 Loss: 0.446 | Acc: 86.315% (22207/25728)\n",
      "300 391 Loss: 0.468 | Acc: 85.556% (32963/38528)\n",
      "0 100 Loss: 1.681 | Acc: 61.000% (61/100)\n",
      "\n",
      "Epoch: 103\n",
      "0 391 Loss: 0.454 | Acc: 85.156% (109/128)\n",
      "100 391 Loss: 0.430 | Acc: 86.734% (11213/12928)\n",
      "200 391 Loss: 0.432 | Acc: 86.641% (22291/25728)\n",
      "300 391 Loss: 0.445 | Acc: 86.137% (33187/38528)\n",
      "0 100 Loss: 1.457 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 104\n",
      "0 391 Loss: 0.473 | Acc: 87.500% (112/128)\n",
      "100 391 Loss: 0.403 | Acc: 87.732% (11342/12928)\n",
      "200 391 Loss: 0.425 | Acc: 87.096% (22408/25728)\n",
      "300 391 Loss: 0.441 | Acc: 86.480% (33319/38528)\n",
      "0 100 Loss: 1.255 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 105\n",
      "0 391 Loss: 0.544 | Acc: 83.594% (107/128)\n",
      "100 391 Loss: 0.397 | Acc: 87.369% (11295/12928)\n",
      "200 391 Loss: 0.425 | Acc: 86.521% (22260/25728)\n",
      "300 391 Loss: 0.441 | Acc: 86.194% (33209/38528)\n",
      "0 100 Loss: 1.835 | Acc: 56.000% (56/100)\n",
      "\n",
      "Epoch: 106\n",
      "0 391 Loss: 0.526 | Acc: 85.156% (109/128)\n",
      "100 391 Loss: 0.407 | Acc: 87.399% (11299/12928)\n",
      "200 391 Loss: 0.416 | Acc: 87.255% (22449/25728)\n",
      "300 391 Loss: 0.425 | Acc: 86.887% (33476/38528)\n",
      "0 100 Loss: 2.078 | Acc: 57.000% (57/100)\n",
      "\n",
      "Epoch: 107\n",
      "0 391 Loss: 0.333 | Acc: 91.406% (117/128)\n",
      "100 391 Loss: 0.386 | Acc: 88.328% (11419/12928)\n",
      "200 391 Loss: 0.384 | Acc: 88.273% (22711/25728)\n",
      "300 391 Loss: 0.406 | Acc: 87.534% (33725/38528)\n",
      "0 100 Loss: 1.487 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 108\n",
      "0 391 Loss: 0.605 | Acc: 82.031% (105/128)\n",
      "100 391 Loss: 0.368 | Acc: 88.946% (11499/12928)\n",
      "200 391 Loss: 0.373 | Acc: 88.577% (22789/25728)\n",
      "300 391 Loss: 0.398 | Acc: 87.744% (33806/38528)\n",
      "0 100 Loss: 1.783 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 109\n",
      "0 391 Loss: 0.367 | Acc: 89.844% (115/128)\n",
      "100 391 Loss: 0.357 | Acc: 89.179% (11529/12928)\n",
      "200 391 Loss: 0.374 | Acc: 88.444% (22755/25728)\n",
      "300 391 Loss: 0.386 | Acc: 88.094% (33941/38528)\n",
      "0 100 Loss: 1.349 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 110\n",
      "0 391 Loss: 0.332 | Acc: 91.406% (117/128)\n",
      "100 391 Loss: 0.359 | Acc: 88.939% (11498/12928)\n",
      "200 391 Loss: 0.366 | Acc: 88.717% (22825/25728)\n",
      "300 391 Loss: 0.383 | Acc: 88.235% (33995/38528)\n",
      "0 100 Loss: 1.257 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 111\n",
      "0 391 Loss: 0.324 | Acc: 88.281% (113/128)\n",
      "100 391 Loss: 0.359 | Acc: 88.699% (11467/12928)\n",
      "200 391 Loss: 0.367 | Acc: 88.697% (22820/25728)\n",
      "300 391 Loss: 0.380 | Acc: 88.229% (33993/38528)\n",
      "0 100 Loss: 1.563 | Acc: 62.000% (62/100)\n",
      "\n",
      "Epoch: 112\n",
      "0 391 Loss: 0.379 | Acc: 88.281% (113/128)\n",
      "100 391 Loss: 0.330 | Acc: 89.991% (11634/12928)\n",
      "200 391 Loss: 0.347 | Acc: 89.199% (22949/25728)\n",
      "300 391 Loss: 0.363 | Acc: 88.673% (34164/38528)\n",
      "0 100 Loss: 1.440 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 113\n",
      "0 391 Loss: 0.407 | Acc: 85.156% (109/128)\n",
      "100 391 Loss: 0.331 | Acc: 89.743% (11602/12928)\n",
      "200 391 Loss: 0.340 | Acc: 89.408% (23003/25728)\n",
      "300 391 Loss: 0.347 | Acc: 89.242% (34383/38528)\n",
      "0 100 Loss: 1.642 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 114\n",
      "0 391 Loss: 0.378 | Acc: 88.281% (113/128)\n",
      "100 391 Loss: 0.331 | Acc: 89.836% (11614/12928)\n",
      "200 391 Loss: 0.331 | Acc: 89.875% (23123/25728)\n",
      "300 391 Loss: 0.344 | Acc: 89.395% (34442/38528)\n",
      "0 100 Loss: 1.909 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 115\n",
      "0 391 Loss: 0.248 | Acc: 91.406% (117/128)\n",
      "100 391 Loss: 0.313 | Acc: 90.254% (11668/12928)\n",
      "200 391 Loss: 0.322 | Acc: 90.093% (23179/25728)\n",
      "300 391 Loss: 0.335 | Acc: 89.610% (34525/38528)\n",
      "0 100 Loss: 1.433 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 116\n",
      "0 391 Loss: 0.303 | Acc: 90.625% (116/128)\n",
      "100 391 Loss: 0.309 | Acc: 90.548% (11706/12928)\n",
      "200 391 Loss: 0.314 | Acc: 90.147% (23193/25728)\n",
      "300 391 Loss: 0.326 | Acc: 89.815% (34604/38528)\n",
      "0 100 Loss: 1.598 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 117\n",
      "0 391 Loss: 0.390 | Acc: 89.844% (115/128)\n",
      "100 391 Loss: 0.302 | Acc: 90.671% (11722/12928)\n",
      "200 391 Loss: 0.300 | Acc: 90.804% (23362/25728)\n",
      "300 391 Loss: 0.314 | Acc: 90.277% (34782/38528)\n",
      "0 100 Loss: 1.457 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 118\n",
      "0 391 Loss: 0.306 | Acc: 91.406% (117/128)\n",
      "100 391 Loss: 0.287 | Acc: 91.035% (11769/12928)\n",
      "200 391 Loss: 0.286 | Acc: 91.091% (23436/25728)\n",
      "300 391 Loss: 0.301 | Acc: 90.620% (34914/38528)\n",
      "0 100 Loss: 1.554 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 119\n",
      "0 391 Loss: 0.233 | Acc: 92.969% (119/128)\n",
      "100 391 Loss: 0.286 | Acc: 91.228% (11794/12928)\n",
      "200 391 Loss: 0.294 | Acc: 90.885% (23383/25728)\n",
      "300 391 Loss: 0.300 | Acc: 90.791% (34980/38528)\n",
      "0 100 Loss: 1.264 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 120\n",
      "0 391 Loss: 0.328 | Acc: 87.500% (112/128)\n",
      "100 391 Loss: 0.275 | Acc: 91.886% (11879/12928)\n",
      "200 391 Loss: 0.277 | Acc: 91.671% (23585/25728)\n",
      "300 391 Loss: 0.289 | Acc: 91.227% (35148/38528)\n",
      "0 100 Loss: 1.513 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 121\n",
      "0 391 Loss: 0.251 | Acc: 92.969% (119/128)\n",
      "100 391 Loss: 0.266 | Acc: 92.041% (11899/12928)\n",
      "200 391 Loss: 0.278 | Acc: 91.581% (23562/25728)\n",
      "300 391 Loss: 0.290 | Acc: 91.165% (35124/38528)\n",
      "0 100 Loss: 1.533 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 122\n",
      "0 391 Loss: 0.304 | Acc: 90.625% (116/128)\n",
      "100 391 Loss: 0.237 | Acc: 92.907% (12011/12928)\n",
      "200 391 Loss: 0.251 | Acc: 92.409% (23775/25728)\n",
      "300 391 Loss: 0.267 | Acc: 91.860% (35392/38528)\n",
      "0 100 Loss: 1.286 | Acc: 65.000% (65/100)\n",
      "\n",
      "Epoch: 123\n",
      "0 391 Loss: 0.272 | Acc: 90.625% (116/128)\n",
      "100 391 Loss: 0.249 | Acc: 92.791% (11996/12928)\n",
      "200 391 Loss: 0.250 | Acc: 92.658% (23839/25728)\n",
      "300 391 Loss: 0.255 | Acc: 92.439% (35615/38528)\n",
      "0 100 Loss: 1.530 | Acc: 64.000% (64/100)\n",
      "\n",
      "Epoch: 124\n",
      "0 391 Loss: 0.279 | Acc: 92.969% (119/128)\n",
      "100 391 Loss: 0.253 | Acc: 92.265% (11928/12928)\n",
      "200 391 Loss: 0.245 | Acc: 92.638% (23834/25728)\n",
      "300 391 Loss: 0.257 | Acc: 92.252% (35543/38528)\n",
      "0 100 Loss: 1.601 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 125\n",
      "0 391 Loss: 0.223 | Acc: 92.188% (118/128)\n",
      "100 391 Loss: 0.218 | Acc: 93.479% (12085/12928)\n",
      "200 391 Loss: 0.232 | Acc: 93.105% (23954/25728)\n",
      "300 391 Loss: 0.241 | Acc: 92.823% (35763/38528)\n",
      "0 100 Loss: 1.345 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 126\n",
      "0 391 Loss: 0.209 | Acc: 92.969% (119/128)\n",
      "100 391 Loss: 0.223 | Acc: 93.332% (12066/12928)\n",
      "200 391 Loss: 0.223 | Acc: 93.144% (23964/25728)\n",
      "300 391 Loss: 0.231 | Acc: 92.948% (35811/38528)\n",
      "0 100 Loss: 1.277 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 127\n",
      "0 391 Loss: 0.257 | Acc: 92.188% (118/128)\n",
      "100 391 Loss: 0.224 | Acc: 93.348% (12068/12928)\n",
      "200 391 Loss: 0.225 | Acc: 93.198% (23978/25728)\n",
      "300 391 Loss: 0.230 | Acc: 93.096% (35868/38528)\n",
      "0 100 Loss: 1.531 | Acc: 63.000% (63/100)\n",
      "\n",
      "Epoch: 128\n",
      "0 391 Loss: 0.224 | Acc: 95.312% (122/128)\n",
      "100 391 Loss: 0.209 | Acc: 93.974% (12149/12928)\n",
      "200 391 Loss: 0.210 | Acc: 93.812% (24136/25728)\n",
      "300 391 Loss: 0.227 | Acc: 93.200% (35908/38528)\n",
      "0 100 Loss: 1.333 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 129\n",
      "0 391 Loss: 0.318 | Acc: 87.500% (112/128)\n",
      "100 391 Loss: 0.174 | Acc: 95.111% (12296/12928)\n",
      "200 391 Loss: 0.189 | Acc: 94.500% (24313/25728)\n",
      "300 391 Loss: 0.203 | Acc: 94.046% (36234/38528)\n",
      "0 100 Loss: 1.478 | Acc: 66.000% (66/100)\n",
      "\n",
      "Epoch: 130\n",
      "0 391 Loss: 0.105 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.183 | Acc: 94.640% (12235/12928)\n",
      "200 391 Loss: 0.185 | Acc: 94.593% (24337/25728)\n",
      "300 391 Loss: 0.190 | Acc: 94.430% (36382/38528)\n",
      "0 100 Loss: 1.335 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 131\n",
      "0 391 Loss: 0.197 | Acc: 93.750% (120/128)\n",
      "100 391 Loss: 0.176 | Acc: 94.910% (12270/12928)\n",
      "200 391 Loss: 0.181 | Acc: 94.644% (24350/25728)\n",
      "300 391 Loss: 0.187 | Acc: 94.396% (36369/38528)\n",
      "0 100 Loss: 1.190 | Acc: 69.000% (69/100)\n",
      "\n",
      "Epoch: 132\n",
      "0 391 Loss: 0.249 | Acc: 93.750% (120/128)\n",
      "100 391 Loss: 0.179 | Acc: 94.640% (12235/12928)\n",
      "200 391 Loss: 0.176 | Acc: 94.772% (24383/25728)\n",
      "300 391 Loss: 0.179 | Acc: 94.703% (36487/38528)\n",
      "0 100 Loss: 1.403 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 133\n",
      "0 391 Loss: 0.145 | Acc: 95.312% (122/128)\n",
      "100 391 Loss: 0.169 | Acc: 95.111% (12296/12928)\n",
      "200 391 Loss: 0.168 | Acc: 95.122% (24473/25728)\n",
      "300 391 Loss: 0.169 | Acc: 95.087% (36635/38528)\n",
      "0 100 Loss: 1.598 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 134\n",
      "0 391 Loss: 0.228 | Acc: 92.188% (118/128)\n",
      "100 391 Loss: 0.166 | Acc: 95.189% (12306/12928)\n",
      "200 391 Loss: 0.163 | Acc: 95.266% (24510/25728)\n",
      "300 391 Loss: 0.166 | Acc: 95.167% (36666/38528)\n",
      "0 100 Loss: 1.393 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 135\n",
      "0 391 Loss: 0.107 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.141 | Acc: 96.086% (12422/12928)\n",
      "200 391 Loss: 0.142 | Acc: 96.020% (24704/25728)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 391 Loss: 0.146 | Acc: 95.845% (36927/38528)\n",
      "0 100 Loss: 1.191 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 136\n",
      "0 391 Loss: 0.096 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.147 | Acc: 95.869% (12394/12928)\n",
      "200 391 Loss: 0.147 | Acc: 95.829% (24655/25728)\n",
      "300 391 Loss: 0.152 | Acc: 95.704% (36873/38528)\n",
      "0 100 Loss: 1.363 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 137\n",
      "0 391 Loss: 0.088 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.127 | Acc: 96.511% (12477/12928)\n",
      "200 391 Loss: 0.129 | Acc: 96.486% (24824/25728)\n",
      "300 391 Loss: 0.135 | Acc: 96.255% (37085/38528)\n",
      "0 100 Loss: 1.282 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 138\n",
      "0 391 Loss: 0.081 | Acc: 97.656% (125/128)\n",
      "100 391 Loss: 0.115 | Acc: 96.937% (12532/12928)\n",
      "200 391 Loss: 0.120 | Acc: 96.755% (24893/25728)\n",
      "300 391 Loss: 0.126 | Acc: 96.571% (37207/38528)\n",
      "0 100 Loss: 1.393 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 139\n",
      "0 391 Loss: 0.118 | Acc: 96.875% (124/128)\n",
      "100 391 Loss: 0.116 | Acc: 97.076% (12550/12928)\n",
      "200 391 Loss: 0.128 | Acc: 96.545% (24839/25728)\n",
      "300 391 Loss: 0.137 | Acc: 96.268% (37090/38528)\n",
      "0 100 Loss: 1.247 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 140\n",
      "0 391 Loss: 0.126 | Acc: 96.094% (123/128)\n",
      "100 391 Loss: 0.115 | Acc: 96.945% (12533/12928)\n",
      "200 391 Loss: 0.115 | Acc: 96.922% (24936/25728)\n",
      "300 391 Loss: 0.117 | Acc: 96.909% (37337/38528)\n",
      "0 100 Loss: 1.225 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 141\n",
      "0 391 Loss: 0.092 | Acc: 99.219% (127/128)\n",
      "100 391 Loss: 0.099 | Acc: 97.587% (12616/12928)\n",
      "200 391 Loss: 0.102 | Acc: 97.380% (25054/25728)\n",
      "300 391 Loss: 0.102 | Acc: 97.360% (37511/38528)\n",
      "0 100 Loss: 1.469 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 142\n",
      "0 391 Loss: 0.114 | Acc: 96.875% (124/128)\n",
      "100 391 Loss: 0.095 | Acc: 97.478% (12602/12928)\n",
      "200 391 Loss: 0.099 | Acc: 97.400% (25059/25728)\n",
      "300 391 Loss: 0.101 | Acc: 97.342% (37504/38528)\n",
      "0 100 Loss: 1.537 | Acc: 67.000% (67/100)\n",
      "\n",
      "Epoch: 143\n",
      "0 391 Loss: 0.181 | Acc: 91.406% (117/128)\n",
      "100 391 Loss: 0.086 | Acc: 97.904% (12657/12928)\n",
      "200 391 Loss: 0.085 | Acc: 97.948% (25200/25728)\n",
      "300 391 Loss: 0.088 | Acc: 97.828% (37691/38528)\n",
      "0 100 Loss: 1.357 | Acc: 68.000% (68/100)\n",
      "\n",
      "Epoch: 144\n",
      "0 391 Loss: 0.055 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.080 | Acc: 98.120% (12685/12928)\n",
      "200 391 Loss: 0.080 | Acc: 98.165% (25256/25728)\n",
      "300 391 Loss: 0.083 | Acc: 98.090% (37792/38528)\n",
      "0 100 Loss: 1.296 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 145\n",
      "0 391 Loss: 0.075 | Acc: 97.656% (125/128)\n",
      "100 391 Loss: 0.064 | Acc: 98.662% (12755/12928)\n",
      "200 391 Loss: 0.067 | Acc: 98.562% (25358/25728)\n",
      "300 391 Loss: 0.070 | Acc: 98.476% (37941/38528)\n",
      "0 100 Loss: 1.450 | Acc: 71.000% (71/100)\n",
      "\n",
      "Epoch: 146\n",
      "0 391 Loss: 0.055 | Acc: 99.219% (127/128)\n",
      "100 391 Loss: 0.057 | Acc: 98.809% (12774/12928)\n",
      "200 391 Loss: 0.056 | Acc: 98.791% (25417/25728)\n",
      "300 391 Loss: 0.057 | Acc: 98.785% (38060/38528)\n",
      "0 100 Loss: 1.185 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 147\n",
      "0 391 Loss: 0.065 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.053 | Acc: 98.840% (12778/12928)\n",
      "200 391 Loss: 0.061 | Acc: 98.655% (25382/25728)\n",
      "300 391 Loss: 0.062 | Acc: 98.640% (38004/38528)\n",
      "0 100 Loss: 1.061 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 148\n",
      "0 391 Loss: 0.025 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.057 | Acc: 98.786% (12771/12928)\n",
      "200 391 Loss: 0.055 | Acc: 98.838% (25429/25728)\n",
      "300 391 Loss: 0.054 | Acc: 98.900% (38104/38528)\n",
      "0 100 Loss: 1.135 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 149\n",
      "0 391 Loss: 0.075 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.051 | Acc: 98.948% (12792/12928)\n",
      "200 391 Loss: 0.048 | Acc: 99.094% (25495/25728)\n",
      "300 391 Loss: 0.049 | Acc: 99.016% (38149/38528)\n",
      "0 100 Loss: 1.077 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 150\n",
      "0 391 Loss: 0.028 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.037 | Acc: 99.335% (12842/12928)\n",
      "200 391 Loss: 0.041 | Acc: 99.234% (25531/25728)\n",
      "300 391 Loss: 0.044 | Acc: 99.151% (38201/38528)\n",
      "0 100 Loss: 1.176 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 151\n",
      "0 391 Loss: 0.034 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.036 | Acc: 99.327% (12841/12928)\n",
      "200 391 Loss: 0.036 | Acc: 99.339% (25558/25728)\n",
      "300 391 Loss: 0.037 | Acc: 99.343% (38275/38528)\n",
      "0 100 Loss: 1.160 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 152\n",
      "0 391 Loss: 0.041 | Acc: 99.219% (127/128)\n",
      "100 391 Loss: 0.032 | Acc: 99.544% (12869/12928)\n",
      "200 391 Loss: 0.031 | Acc: 99.580% (25620/25728)\n",
      "300 391 Loss: 0.031 | Acc: 99.559% (38358/38528)\n",
      "0 100 Loss: 1.212 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 153\n",
      "0 391 Loss: 0.036 | Acc: 99.219% (127/128)\n",
      "100 391 Loss: 0.030 | Acc: 99.559% (12871/12928)\n",
      "200 391 Loss: 0.030 | Acc: 99.569% (25617/25728)\n",
      "300 391 Loss: 0.030 | Acc: 99.561% (38359/38528)\n",
      "0 100 Loss: 1.128 | Acc: 72.000% (72/100)\n",
      "\n",
      "Epoch: 154\n",
      "0 391 Loss: 0.011 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.025 | Acc: 99.636% (12881/12928)\n",
      "200 391 Loss: 0.026 | Acc: 99.642% (25636/25728)\n",
      "300 391 Loss: 0.025 | Acc: 99.665% (38399/38528)\n",
      "0 100 Loss: 0.990 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 155\n",
      "0 391 Loss: 0.012 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.020 | Acc: 99.768% (12898/12928)\n",
      "200 391 Loss: 0.021 | Acc: 99.755% (25665/25728)\n",
      "300 391 Loss: 0.021 | Acc: 99.769% (38439/38528)\n",
      "0 100 Loss: 1.053 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 156\n",
      "0 391 Loss: 0.060 | Acc: 98.438% (126/128)\n",
      "100 391 Loss: 0.021 | Acc: 99.768% (12898/12928)\n",
      "200 391 Loss: 0.020 | Acc: 99.790% (25674/25728)\n",
      "300 391 Loss: 0.020 | Acc: 99.803% (38452/38528)\n",
      "0 100 Loss: 0.975 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 157\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.014 | Acc: 99.930% (12919/12928)\n",
      "200 391 Loss: 0.015 | Acc: 99.907% (25704/25728)\n",
      "300 391 Loss: 0.016 | Acc: 99.883% (38483/38528)\n",
      "0 100 Loss: 1.086 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 158\n",
      "0 391 Loss: 0.017 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.013 | Acc: 99.930% (12919/12928)\n",
      "200 391 Loss: 0.013 | Acc: 99.930% (25710/25728)\n",
      "300 391 Loss: 0.014 | Acc: 99.904% (38491/38528)\n",
      "0 100 Loss: 1.046 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 159\n",
      "0 391 Loss: 0.006 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.014 | Acc: 99.899% (12915/12928)\n",
      "200 391 Loss: 0.014 | Acc: 99.891% (25700/25728)\n",
      "300 391 Loss: 0.013 | Acc: 99.909% (38493/38528)\n",
      "0 100 Loss: 0.977 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 160\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.012 | Acc: 99.938% (12920/12928)\n",
      "200 391 Loss: 0.012 | Acc: 99.930% (25710/25728)\n",
      "300 391 Loss: 0.012 | Acc: 99.927% (38500/38528)\n",
      "0 100 Loss: 0.945 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 161\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.011 | Acc: 99.938% (12920/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.957% (25717/25728)\n",
      "300 391 Loss: 0.011 | Acc: 99.953% (38510/38528)\n",
      "0 100 Loss: 1.067 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 162\n",
      "0 391 Loss: 0.011 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.011 | Acc: 99.930% (12919/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.938% (25712/25728)\n",
      "300 391 Loss: 0.011 | Acc: 99.940% (38505/38528)\n",
      "0 100 Loss: 1.065 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 163\n",
      "0 391 Loss: 0.010 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.961% (12923/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.946% (25714/25728)\n",
      "300 391 Loss: 0.011 | Acc: 99.943% (38506/38528)\n",
      "0 100 Loss: 0.942 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 164\n",
      "0 391 Loss: 0.011 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.011 | Acc: 99.961% (12923/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.946% (25714/25728)\n",
      "300 391 Loss: 0.011 | Acc: 99.953% (38510/38528)\n",
      "0 100 Loss: 0.965 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 165\n",
      "0 391 Loss: 0.010 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.011 | Acc: 99.923% (12918/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.922% (25708/25728)\n",
      "300 391 Loss: 0.011 | Acc: 99.933% (38502/38528)\n",
      "0 100 Loss: 0.971 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 166\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.961% (12923/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.957% (25717/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.956% (38511/38528)\n",
      "0 100 Loss: 0.950 | Acc: 78.000% (78/100)\n",
      "\n",
      "Epoch: 167\n",
      "0 391 Loss: 0.010 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.011 | Acc: 99.954% (12922/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.942% (25713/25728)\n",
      "300 391 Loss: 0.011 | Acc: 99.943% (38506/38528)\n",
      "0 100 Loss: 0.929 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 168\n",
      "0 391 Loss: 0.006 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.966% (38515/38528)\n",
      "0 100 Loss: 0.990 | Acc: 73.000% (73/100)\n",
      "\n",
      "Epoch: 169\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 391 Loss: 0.011 | Acc: 99.954% (12922/12928)\n",
      "200 391 Loss: 0.011 | Acc: 99.953% (25716/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.956% (38511/38528)\n",
      "0 100 Loss: 0.915 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 170\n",
      "0 391 Loss: 0.010 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.977% (25722/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.966% (38515/38528)\n",
      "0 100 Loss: 0.906 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 171\n",
      "0 391 Loss: 0.011 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.915% (12917/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.934% (25711/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.940% (38505/38528)\n",
      "0 100 Loss: 0.903 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 172\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.961% (12923/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.961% (25718/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.951% (38509/38528)\n",
      "0 100 Loss: 0.933 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 173\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.965% (25719/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.964% (38514/38528)\n",
      "0 100 Loss: 0.899 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 174\n",
      "0 391 Loss: 0.010 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.954% (12922/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.957% (25717/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.953% (38510/38528)\n",
      "0 100 Loss: 0.897 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 175\n",
      "0 391 Loss: 0.011 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.965% (25719/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.964% (38514/38528)\n",
      "0 100 Loss: 0.923 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 176\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.971% (38517/38528)\n",
      "0 100 Loss: 0.906 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 177\n",
      "0 391 Loss: 0.010 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.977% (25722/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.964% (38514/38528)\n",
      "0 100 Loss: 0.893 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 178\n",
      "0 391 Loss: 0.011 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.946% (12921/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.957% (25717/25728)\n",
      "300 391 Loss: 0.010 | Acc: 99.961% (38513/38528)\n",
      "0 100 Loss: 0.922 | Acc: 79.000% (79/100)\n",
      "\n",
      "Epoch: 179\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.969% (38516/38528)\n",
      "0 100 Loss: 0.910 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 180\n",
      "0 391 Loss: 0.006 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.985% (12926/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.977% (25722/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.966% (38515/38528)\n",
      "0 100 Loss: 0.920 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 181\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.961% (12923/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.965% (25719/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.971% (38517/38528)\n",
      "0 100 Loss: 0.910 | Acc: 78.000% (78/100)\n",
      "\n",
      "Epoch: 182\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.985% (12926/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.981% (25723/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.969% (38516/38528)\n",
      "0 100 Loss: 0.911 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 183\n",
      "0 391 Loss: 0.010 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 100.000% (12928/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.984% (25724/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.982% (38521/38528)\n",
      "0 100 Loss: 0.931 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 184\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.977% (25722/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.977% (38519/38528)\n",
      "0 100 Loss: 0.901 | Acc: 78.000% (78/100)\n",
      "\n",
      "Epoch: 185\n",
      "0 391 Loss: 0.012 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.985% (12926/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.984% (25724/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.982% (38521/38528)\n",
      "0 100 Loss: 0.934 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 186\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.985% (12926/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.984% (25724/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.974% (38518/38528)\n",
      "0 100 Loss: 0.924 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 187\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.010 | Acc: 99.938% (12920/12928)\n",
      "200 391 Loss: 0.010 | Acc: 99.949% (25715/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.964% (38514/38528)\n",
      "0 100 Loss: 0.922 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 188\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.981% (25723/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.979% (38520/38528)\n",
      "0 100 Loss: 0.925 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 189\n",
      "0 391 Loss: 0.012 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.979% (38520/38528)\n",
      "0 100 Loss: 0.927 | Acc: 75.000% (75/100)\n",
      "\n",
      "Epoch: 190\n",
      "0 391 Loss: 0.014 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.985% (12926/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.981% (25723/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.979% (38520/38528)\n",
      "0 100 Loss: 0.902 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 191\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.954% (12922/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.965% (25719/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.971% (38517/38528)\n",
      "0 100 Loss: 0.908 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 192\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.961% (12923/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.981% (25723/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.977% (38519/38528)\n",
      "0 100 Loss: 0.926 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 193\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.974% (38518/38528)\n",
      "0 100 Loss: 0.929 | Acc: 74.000% (74/100)\n",
      "\n",
      "Epoch: 194\n",
      "0 391 Loss: 0.010 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.973% (25721/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.979% (38520/38528)\n",
      "0 100 Loss: 0.895 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 195\n",
      "0 391 Loss: 0.009 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.008 | Acc: 99.977% (12925/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.965% (25719/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.966% (38515/38528)\n",
      "0 100 Loss: 0.911 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 196\n",
      "0 391 Loss: 0.008 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.977% (25722/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.977% (38519/38528)\n",
      "0 100 Loss: 0.921 | Acc: 78.000% (78/100)\n",
      "\n",
      "Epoch: 197\n",
      "0 391 Loss: 0.010 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.985% (12926/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.984% (25724/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.982% (38521/38528)\n",
      "0 100 Loss: 0.900 | Acc: 77.000% (77/100)\n",
      "\n",
      "Epoch: 198\n",
      "0 391 Loss: 0.006 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.985% (12926/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.984% (25724/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.987% (38523/38528)\n",
      "0 100 Loss: 0.922 | Acc: 76.000% (76/100)\n",
      "\n",
      "Epoch: 199\n",
      "0 391 Loss: 0.007 | Acc: 100.000% (128/128)\n",
      "100 391 Loss: 0.009 | Acc: 99.969% (12924/12928)\n",
      "200 391 Loss: 0.009 | Acc: 99.969% (25720/25728)\n",
      "300 391 Loss: 0.009 | Acc: 99.966% (38515/38528)\n",
      "0 100 Loss: 0.910 | Acc: 77.000% (77/100)\n"
     ]
    }
   ],
   "source": [
    "args.block = \"NEW_1\"\n",
    "net = ResNet18(block=args.block, num_classes=100 if args.dataset == 'cifar100' else 10)\n",
    "ours_accuracy = run_model(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c69c248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(resnet_accuracy, color = 'r')\n",
    "#plt.plot(se_sa_accuracy, color = 'y')\n",
    "#plt.plot(sec_sa_accuracy, color = 'b')\n",
    "#plt.plot(cbam_accuracy, color = 'm')\n",
    "plt.plot(ours_accuracy, color = 'g')\n",
    "plt.xlim([140, 200])      # X축의 범위: [xmin, xmax]\n",
    "plt.ylim([75, 80])     # Y축의 범위: [ymin, ymax]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831ae3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
